\documentclass[11pt,draftcls, onecolumn]{IEEEtran}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{color}
\usepackage[pdftex]{hyperref}
\usepackage{epstopdf}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{latexsym}
\usepackage{multirow}
% \usepackage[caption=false]{subfig}

\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
\fi


% new command section, Thanks for JMO for the suggestion to keep the document clean and unified.

\include{Newcommand}


\begin{document}

\title{Deep Dynamic Neural Networks for Multimodal Gesture
Segmentation and Recognition}

\author{Di~Wu,
        Lionel~Pigou,
        Pieter-Jan Kindermans,
        Nam~Le,
        Ling~Shao,
        Joni Dambre,
        and~Jean-Marc Odobez
%\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem M. Shell is with the Department
%of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
%GA, 30332.\protect\\
%% note need leading \protect in front of \\ to get a newline within \thanks as
%% \\ is fragile and will error, could use \hfil\break instead.
%E-mail: see http://www.michaelshell.org/contact.html
%\IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.}% <-this % stops an unwanted space
%\thanks{Manuscript received April 19, 2005; revised September 17, 2014.}
}


% The paper headers
\markboth{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, December~2014}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}


\IEEEtitleabstractindextext{%
\begin{abstract}
This paper describes a novel method called deep dynamic neural networks \emph{(DDNN)} for  multimodal gesture recognition.
A semi-supervised hierarchical dynamic framework is proposed for simultaneous gesture segmentation and recognition taking skeleton, depth and RGB images as input observations.
Unlike the traditional construction of complex handcrafted features, all inputs are learnt by deep neural networks: the skeletal module is modelled by deep belief networks \emph{(DBN)}; the depth and RGB module are modelled by 3D convolutional neural networks \emph{(3DCNN)} to extract high-level spatio-temporal features.
The learned representations are then used for estimating emission probabilities of the hidden Markov models to infer a gesture sequence.
The framework can be easily extended by including an ergodic state to segment and recognise video sequences by a frame-to-frame mechanism, making online segmentation and recognition possible.
This purely data driven approach achieves a score of \textbf{\emph{0.81}} in the ChaLearn LAP gesture spotting challenge. The performance is on par with a variety of the state-of-the-art hand-tuned feature approaches and other learning based methods, opening the doors for using deep learning techniques to explore multimodal time series.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Deep learning, convolutional neural networks, deep belief networks, hidden Markov models, 
gesture recognition.
\end{IEEEkeywords}}

\maketitle
\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle


%\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\input{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{RelatedWork}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Approach}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{ModelDetails}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Results}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Discussion}

Hand-engineered, task-specific features are often less adaptive and time-consuming to design. This difficulty is more pronounced with multimodal data as the features have to relate with multiple data sources.
In this paper, we presented a novel deep dynamic neural network (DDNN) framework that utilises deep belief betworks and 3D convolutional neural networks for learning contextual frame-level representations and modelling emission probabilities for Markov fields.
 The heterogeneous inputs from skeletal joints, RGB and depth images require different feature learning methods and the late fusion scheme is adopted at the score level. The experimental results on bi-modal time series data show that the multimodal DDNN framework can learn a good model of the joint space of multiple sensory inputs, and is consistently as good as or better than the unimodal input, opening the door for exploring the complementary representation among multimodal inputs. It also suggests that learning features directly from data is a very important research direction and with more and more data and flops-free  computational power, the learning-based methods are not only more generalisable to many domains, but also are powerful in combining with other well-studied probabilistic graphical models for modelling and reasoning dynamic sequences.
 \dwucomline{Should we mention the below?}
Future works include learning the share representation amongst the heterogeneous inputs in a more meaningful fashion as in~\cite{neverova2014moddrop} and integrate the overall system into a unified deep learning framework by replacing the temporal modelling part with Recurrent Neural Nets such as the work of ~\cite{DBLP:journals/corr/PigouODHD15}.

\appendices
% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{Details of the Code}
The python code using Theano~\cite{Bastien-Theano-2012} for this work can be found at: \\
\footnotesize{\verb+https://github.com/stevenwudi/chalearn2014_wudi_lio+}


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi

The authors would like to thank Sander Dieleman for his guidance in building, training and initialising convolutional neural networks.


\bibliographystyle{IEEEtran}
\bibliography{tPAMI2015}

\newpage

\newpage

\input{ReviewersComments}


\end{document}
