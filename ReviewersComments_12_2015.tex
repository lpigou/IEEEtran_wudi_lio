\newcommand{\rev}[1]{{\noindent {\bf Comment:} {\it #1}}~\\[-2mm]}
\newcommand{\ans}[1]{{\noindent {\bf Response:} #1}~\\[-2mm]}
\newcommand{\td}[1]{{\noindent {\bf TODO:} #1}~\\}


\newcommand{\PM}[1]{
~\\[-4mm]
%``\dots{\em #1}\ldots''
``{\em #1}''
}


% -----------------------------------------------------------------------------------
\newpage
{\LARGE \noindent Response to Reviewer 1}\newline

\rev{
After reading the responses and revised paper, I am happy to say that my confusions and criticisms are rebutted. I thank the authors for their diligence in indulging my requests for clarification and additional experiments. I believe the paper is much improved and should be accepted.

Specifically:

- The additional discussion convinced me of sufficient contribution (jointly learning features and emission probabilities from heterogeneous input streams for gesture recognition).
	
- I agree with the authors that ``More investigation on how to handle heterogeneous networks should be conducted". The additional experiments have identified important problems
(e.g. differences in mean activation) for future work, which is a contribution in itself. In Sec 5.3, Late vs. Intermediate fusion: If done right, the intermediate fusion should work at-least-as-well as the late fusion? But, as the authors discuss, it is not trivial to `do it right'. I think they have done their due-diligence here and, importantly, exposed a problem.

- The qualitative analysis is greatly improved, giving insight.

- I am happy to see intermediate fusion and fine tuning have been considered.

 After reading the rebuttal and spending some time searching the literature I agree the work is new and interesting. The authors identify problems and
challenges for future work. It isn't a revolutionary paper, but it seems an important and valid milestone which contributes understanding to our community.
}

\ans{
Thank you for your review and recognition of the improvement of the revised paper. The suggestions from the reviewers are also valuable resources for us to better understand the proposed algorithm and make better reflective experimental analysis.
}


\rev{
Below I enumerate my few remaining concerns, which I believe the authors
should consider in a minor revision:
A few minor substantive points:
4.1: You might consider a latent variable model for the state labels $y_{i,t}$ instead of `force alignment'. It might help to briefly justify why you think forced assignment is good enough.
}

\ans{
\td{}
Thank you for your insightful comments. We did encounter similar thoughts when implementing the system as previously some speech processing researchers have done. 
[\td{We have incorporated a brief discussion in Sec....}] However, with the re-alignment of the label boundary could potentially improve the result, we didn't digress to this route as another potential contribution of the paper.
}

\rev{
Figure 8: Label the y-axis on the figure. The information is in the caption, but readers are lazy.
}


\ans{
We have updated the annotated x-axis and y-axis figure for Figure 8.
}

\rev{
A few minor grammatical problems:
The authors jointly learn features and emission probabilities from
*heterogeneous input streams* for dynamic gesture recognition.
}

\ans{
Thank you for the careful review. We have corrected some grammatical problems during this review.
}


% -----------------------------------------------------------------------------------
\newpage
{\LARGE \noindent Response to Reviewer 2}\newline

\rev{
The authors made an excellent effort to improve the paper based on all the reviewers' comments. The paper proposes the fusion of the output from a Gaussian-Bernoulli Deep Belief network operating on skeletal features and the output of a Convolutional Neural Network operating on RGBD data to perform gesture segmentation and recognition.  The paper advances the field of gesture recognition by using both data sources and deep learning architectures within a Hidden Markov Model chain. The results are improved compared to using either architecture independently.

The revised version of the paper includes another fusion scheme: instead of averaging the outputs of the two networks (called ``late fusion"), the paper proposes to concatenate the high-level representations produced by their penultimate layers and process them through another classification layer to get the emission probabilities (called ``intermediate fusion"). Unfortunately, this type of fusion, which made more sense to me, performs slightly worse. I suspect that the reason is that different types of units (\emph{ReLU} vs sigmoid units) are used in the two networks causing incompatible scales in the outputs of the penultimate layers, as the authors indicate. In any case, this add-on is useful and can inspire future investigation on this topic and better implementations.
I recommend acceptance.
}

\ans{
Thank you for your review and positive outlook the revised paper. The suggestions from the reviewers were also a stimulus spur for us have a better analysis of the proposed method. We also agree that our fusion method exposes a problem for future investigation which is also a contribution in itself.
}


\rev{
I recommend the authors to make an additional pass to correct typos e.g. combinatin $\rightarrow$ combination.
}

\ans{
Thank you for your careful review. We have proof read the paper and corrected the grammatical mistakes accordingly.
}

% -----------------------------------------------------------------------------------
\newpage
{\LARGE \noindent Response to Reviewer 3}\newline

\rev{
In this second revision, readability and organization of the manuscript have been significantly improved. Additional experiments have been conducted, and interesting insights and discussions have been added, including comparison of intermediate vs late fusion and temporal modeling vs aggregation of per-frame predictions by voting. Finally, more consistent and complete overview of related work is included.
There are no changes in the method though: it remains interesting from a practical point of view, but not particularly novel.
}


\ans{
Thank you for reviewer's recognition of the improvement of the readability and organisation of the paper. The reviewing process also helps us to better understand, analysis the proposed model. 
From the reviewing process and the experiments conducting for intermediate fusion, we investigate the difference of mean activations in intermediate fusion due to different activation functions is analysed which is a contribution itself so as to spur further investigation to effectively fuse multi-model, various activations.
}

\rev{
In section II, please note that a year earlier, ChaLearn 2013 competition was dominated by exploiting HMMs (also RNNs) for the same task and on the same dataset.
http://gesture.chalearn.org/2013-multi-modal-challenge/workshop-2013-challenge
Some works are published:
\begin{itemize}
\item HMM:Fusing multi-modal features for gesture recognition, ICMI workshop, 2013
\item HMM:Nandakumar et al., A Multi-modal Gesture Recognition System Using Audio, Video, and Skeletal Joint Data, ICMI workshop, 2013
\item RNN:Neverova et al., A multi-scale approach to gesture detection and recognition, ICCV workshop, 2013
\end{itemize}
}


\ans{
Thank you for the suggested related works. They are very relevant to the paper and the problem that the proposed system is trying to solve. We have included the discussions of the suggested works in the Section: ``Nandakumar \emph{et al.}~\cite{nandakumar2013multi} applied the MFCC+HMM paradigm for audio input, Space-Time-Interest-Point (STIP), Bag-of-Words (BoW) and SVM pipeline for RGB videos and a covariance descriptor with SVM for skeleton module prediction. The 1st ranked team by Wu \emph{et al.}~\cite{wu2013fusing} used HMM model as audio feature classifier and Dynamic Time Warping as the classifier for skeleton feature.  The Recurrent Neural Network (RNN) was deployed in~\cite{neverova2013multi} to model large-scale temporal dependencies, data fusion and gesture classification.  Interestingly, the system in~\cite{neverova2013multi} decomposed the gestures into a large-scale body motion and local subtle movements."
The key differences between the aforementioned papers and the proposed approach is that we use HMM for modelling hidden stats of gesture over the joint feature space whilst their HMM models are purely for audio input~\cite{nandakumar2013multi,wu2013fusing}. Our proposed system uses DBN with pre-training to learn the skeleton features instead of the hand crafted features~\cite{neverova2013multi}. Moreover, we explore the late and intermediate fusion scheme instead of the weighted likelihood that is adopted by~\cite{nandakumar2013multi}. Albeit the intermediate fusion scheme does not outperform late fusion, the discrepancy is a contribution in itself.
}

\rev{
Some sections of the manuscript require additional proof reading: please check for typos (they are many, especially in section III), verb endings and spelling of names in your references.
}

\ans{
Thank you for your careful review. We have proof read the paper and corrected the grammatical mistakes accordingly.
}

\rev{
In terms of performance, the described DBN for treating raw skeleton data looks promising (16.5\% validation error), while 3D Convnets on depth and intensity images perform, unfortunately, poorly (39\% validation error), especially taking into account that two kinds of input (hands and full body) are combined. Why not get some insights from existing state of the art architectures (from [57], for example), and further improve them by adding full body processing and better modeling of temporal dependencies?

The authors propose a deep learning framework for multi-modal gesture recognition based on color, depth and skeleton streams provided by the Kinect sensor. One of the key contributions is combining feature learning with HMM-based temporal modeling. Although exploring the deep learning approaches in the given context is certainly promising, the proposed implementation is rather straightforward and the reported results are clearly behind the state-of-the-art.}

\ans{
\td{}
}

\endinput
