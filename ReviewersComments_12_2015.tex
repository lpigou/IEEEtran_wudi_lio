\newcommand{\rev}[1]{{\noindent {\bf Comment:} {\it #1}}~\\[-2mm]}
\newcommand{\ans}[1]{{\noindent {\bf Response:} #1}~\\[-2mm]}
\newcommand{\td}[1]{{\noindent {\bf TODO:} #1}~\\}


\newcommand{\PM}[1]{
~\\[-4mm]
%``\dots{\em #1}\ldots''
``{\em #1}''
}


% -----------------------------------------------------------------------------------
\newpage
{\LARGE \noindent Response to Reviewer 1}\newline

\rev{
After reading the responses and revised paper, I am happy to say that my confusions and criticisms are rebutted. I thank the authors for their diligence in indulging my requests for clarification and additional experiments. I believe the paper is much improved and should be accepted. 

Specifically:

- The additional discussion convinced me of sufficient contribution (jointly learning features and emission probabilities from heterogeneous input streams for gesture recognition).
	
- I agree with the authors that ``More investigation on how to handle heterogeneous networks should be conducted". The additional experiments have identified important problems
(e.g. differences in mean activation) for future work, which is a contribution in itself. In Sec 5.3, Late vs. Intermediate fusion: If done right, the intermediate fusion should work at-least-as-well as the late fusion? But, as the authors discuss, it is not trivial to `do it right'. I think they have done their due-diligence here and, importantly, exposed a problem.

- The qualitative analysis is greatly improved, giving insight.

- I am happy to see intermediate fusion and fine tuning have been considered.

 After reading the rebuttal and spending some time searching the literature I agree the work is new and interesting. The authors identify problems and
challenges for future work. It isn't a revolutionary paper, but it seems an important and valid milestone which contributes understanding to our community.
}

\ans{
Thank you for your review and recognition of the improvement of the revised paper. The suggestions from the reviewers are also valuable resources for us to better understand the proposed algorithm and make better reflective experimental analysis.
}


\rev{
Below I enumerate my few remaining concerns, which I believe the authors
should consider in a minor revision:
A few minor substantive points:
4.1: You might consider a latent variable model for the state labels $y_{i,t}$ instead of `force alignment'. It might help to briefly justify why you think forced assignment is good enough.
}

\ans{
\td{...}
}

\rev{
Figure 8: Label the y-axis on the figure. The information is in the caption, but readers are lazy.
}


\ans{
We have updated the annotated x-axis and y-axis figure for Figure 8.
}

\rev{
A few minor grammatical problems:
The authors jointly learn features and emission probabilities from
*heterogeneous input streams* for dynamic gesture recognition.
}

\ans{
Thank you for the careful review. We have corrected some grammatical problems during this review.
}


% -----------------------------------------------------------------------------------
\newpage
{\LARGE \noindent Response to Reviewer 2}\newline

\rev{
The authors made an excellent effort to improve the paper based on all the reviewers' comments. The paper proposes the fusion of the output from a Gaussian-Bernoulli Deep Belief network operating on skeletal features and the output of a Convolutional Neural Network operating on RGBD data to perform gesture segmentation and recognition.  The paper advances the field of gesture recognition by using both data sources and deep learning architectures within a Hidden Markov Model chain. The results are improved compared to using either architecture independently.

The revised version of the paper includes another fusion scheme: instead of averaging the outputs of the two networks (called ``late fusion"), the paper proposes to concatenate the high-level representations produced by their penultimate layers and process them through another classification layer to get the emission probabilities (called ``intermediate fusion"). Unfortunately, this type of fusion, which made more sense to me, performs slightly worse. I suspect that the reason is that different types of units (\emph{ReLU} vs sigmoid units) are used in the two networks causing incompatible scales in the outputs of the penultimate layers, as the authors indicate. In any case, this add-on is useful and can inspire future investigation on this topic and better implementations.
I recommend acceptance.
}

\ans{
Thank you for your review and positive outlook the revised paper. The suggestions from the reviewers were also a stimulus spur for us have a better analysis of the proposed method. We also agree that our fusion method exposes a problem for future investigation which is also a contribution in itself.
}


\rev{
I recommend the authors to make an additional pass to correct typos e.g. combinatin $\rightarrow$ combination.
}

\ans{
Thank you for your careful review. We have proof read the paper and corrected the grammatical mistakes accordingly.
}

% -----------------------------------------------------------------------------------
\newpage
{\LARGE \noindent Response to Reviewer 3}\newline

\rev{
In this second revision, readability and organization of the manuscript have been significantly improved. Additional experiments have been conducted, and interesting insights and discussions have been added, including comparison of intermediate vs late fusion and temporal modeling vs aggregation of per-frame predictions by voting. Finally, more consistent and complete overview of related work is included.
There are no changes in the method though: it remains interesting from a practical point of view, but not particularly novel.
}


\ans{
\td{}
}

\rev{
In section II, please note that a year earlier, ChaLearn 2013 competition was dominated by exploiting HMMs (also RNNs) for the same task and on the same dataset.
http://gesture.chalearn.org/2013-multi-modal-challenge/workshop-2013-challenge
Some works are published:
[HMM] Fusing multi-modal features for gesture recognition, ICMI workshop, 2013
[HMM] Nandakumar et al., A Multi-modal Gesture Recognition System Using Audio, Video, and Skeletal Joint Data, ICMI workshop, 2013
[RNN] Neverova et al., A multi-scale approach to gesture detection and recognition, ICCV workshop, 2013
}


\rev{
Some sections of the manuscript require additional proof reading: please check for typos (they are many, especially in section III), verb endings and spelling of names in your references.
}

\ans{
\td{}
}

\rev{
In terms of performance, the described DBN for treating raw skeleton data looks promising (16.5\% validation error), while 3D Convnets on depth and intensity images perform, unfortunately, poorly (39\% validation error), especially taking into account that two kinds of input (hands and full body) are combined. Why not get some insights from existing state of the art architectures (from [57], for example), and further improve them by adding full body processing and better modeling of temporal dependencies?

The authors propose a deep learning framework for multi-modal gesture recognition based on color, depth and skeleton streams provided by the Kinect sensor. One of the key contributions is combining feature learning with HMM-based temporal modeling. Although exploring the deep learning approaches in the given context is certainly promising, the proposed implementation is rather straightforward and the reported results are clearly behind the state-of-the-art.}

\ans{
\td{}
}

\endinput
