\newcommand{\rev}[1]{{\noindent {\bf Comment:} {\it #1}}~\\[-2mm]}
\newcommand{\ans}[1]{{\noindent {\bf Response:} #1}~\\[-2mm]}
\newcommand{\td}[1]{{\noindent {\bf TODO:} #1}~\\}



\newcommand{\PM}[1]{
~\\[-4mm]
%``\dots{\em #1}\ldots''
``{\em #1}''
}


% -----------------------------------------------------------------------------------
% -----------------------------------------------------------------------------------
\newpage
{\LARGE \noindent Response to Associate Editor}\newline



\rev{Reviews of the paper have been received. In general comments are positive. Still one reviewer suggests some interesting extra analyses of the proposed method. Please carefully address all reviewers comments for a second review round of the paper. Please provide your revision by July 31th.}

\ans{We would  like to thank the associate editor's general positive comments on our paper. We also would like to thank reviewers' careful and insightful suggestions for improving the paper. Following are some common points that were mentioned by the reviewers and we list the most notable changes below. The point to point responses are provided afterwards.

\begin{itemize}
\item In the previous version for multimodal fusion, we used the late fusion scheme  $s = a * s1 + (1-a)*s2$ where $a$ is chosen by cross-validation. In this revision, we implement an early fusion scheme in Section~\ref{early_fusion} that a new-top level perceptron layer is created to combine two models' output as in Fig.~\ref{fig:fusion}. The new multimodal neural network's parameters are initialised by the previously trained individual module, taking advantage of different modules intrinsic properties and making the network converge much faster. The early fusion system uses pre-trained weights. The results are reported in  Section~\ref{early_fusion} and Table~\ref{Table_score_fusion}.

\item The Related Works section has been moved after the Introduction section. We also follow reviewers' suggestions and include discussions of related works concerning works of: 1) exploiting temporal models in the context of gesture recognition; 2) literature for \RGBD data using deep learning -
    ``Wang \emph{et al.}~\cite{wang2006hidden} introduced a discriminative hidden-state approach for the recognition of human gestures. However, their discriminative training is limited for pre-segmented gesture sequences and one layer of hidden states might not learn powerful enough high level representations for a larger corpus."
    ``In the field of deep learning from RGBD data, Socher \emph{et al.}~\cite{socher2012convolutional} proposed a single convolutional neural net layer for each modality as input to multiple, fixed-tree RNNs in order to compose higher order features for 3D object classification. The single convolutional neural net layer provides useful translational invariance of low level features such as edges and allows parts of an object to be deformable to some extent. The pooled filter responses are then given to a recursive neural network to learn compositional features and part interactions.
    Gupta \emph{et al.}~\cite{gupta2014learning} proposed a geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. The depth image was represented by three channels: horizontal disparity, height above ground and angle with gravity. This augmented representation allows the CNN to learn strong features than by using disparity (or depth) alone. "

\item Experimental analysis. We have included some time analysis in Section~\ref{sec:ComputationalComplexity} and visualisation of response maps after learnt filters in Fig~\ref{3dcnn_filters}.

\item Explanation of intuition behind higher level presentation of the skeleton features. We include Section \ref{sec:ProblemFormation} to explain the intuition behind higher level representation for skeleton joint features which appeared in our previous CVPR paper but was not included in the previous submission. We think this part is one of major contributions of the paper and inclusion of this section makes the journal paper more self-contained.
\end{itemize}
}

% -----------------------------------------------------------------------------------
\newpage
{\LARGE \noindent Response to Reviewer 1}\newline

We thank the reviewer for his time and comments and positive appreciation of the paper.
Below, we provide our answers to his comments and to the unclear points that were raised,
and what we have done to clarify the paper and take comments into account.
Note that in our answer, all references (Equations, Figures)
refer to the new version unless stated otherwise.

\qquad

\rev{The article is easy to read and well structured. The methodology is not strictly novel but its application in the gesture domain with the multimodal fusion makes the article worth reading. Although results are arguably a little behind the maximum performance ones the overall impression of the article is favorable and I believe the community may benefit to check the ideas included in this paper.

The article proposes a framework for dynamic data augmenting a HMM with deep learning techniques and apply this to gesture segmentation and recognition. Gestures segmentation and recognition is a difficult problem. The article tackles this difficulty by means of pure data driven approaches similar to the ones used for speech recognition. The particularities of the computer vision domain are handled accordingly.}

\ans{Thank you for your review and positive outlook of the paper. We are also aware that our results are arguably a little behind the maximum performance, and this may be due to the network initialisation and multimodal neural network learning. 

Note that the revised version has undergone an important rewriting which hopefully should further improve the clarity of the method description, as well as the 
DBN and 3DCNN motivation (see Section III.C).
It also includes extra experimental analysis and an extra early fusion implementation and evaluation  to further extend the broadness of the paper.
}

% -----------------------------------------------------------------------------------
\newpage

{\LARGE \noindent Response to Reviewer 2}\newline

We thank the reviewer for his time and comments.
Note that the article has been reworked significantly to better introduce and enhance the modeling elements and their motivation (see Section III.C).
%
In addition, further experiments wit an early fusion scheme learning a joint multimodal representation have been conducted.
%
Below, we provide our specific answers to his comments and to the unclear points that were raised,
and what we have done to clarify the paper and take comments into account.
%
Note that in our answer, all references (Equations, Figures)
refer to the new version unless stated otherwise.

\vspace{5mm}

\rev{In general, the manuscript is well written and is easy to follow. In the given case, it would be preferable to have the "Related work" section right after the introduction, as otherwise paper's contributions are not completely clear. Furthermore, there is certainly a vast literature on exploiting HMMs in the context of gesture recognition (as well as other temporal models, such as recurrent neural networks), which should be briefly summarized, the differences with the proposed solution should be highlighted.}

\ans{
Thank you for your comments and the recognition of easy readability of the paper. We have moved the related work section after the introduction section. Moreover, we have included the discussions of literature that utilise temporal models, e.g.,``Wang \emph{et al.}~\cite{wang2006hidden} introduced a discriminative hidden-state approach for the recognition of human gestures. However, their discriminative training is limited for pre-segmented gesture sequences and one layer of hidden states might not learn powerful enough high level representations for a larger corpus",  ``~\cite{socher2012convolutional} proposed a single convolutional neural net layer for each modality as input to multiple, fixed-tree RNNs in order to compose higher order features for 3D object classification."
}

\vspace{5mm}

\rev{Authors claim to learn a model in the joint multi-modal space is a slight overstatement, as neural networks processing different modalities are trained completely independently with following averaging of produced scores.}

\ans{
Thank you for your comments. In this revision, we implement the early fusion scheme in Section~\ref{early_fusion} that ``we adopt another layer of perceptron for cross modality learning taking the input from each individual net's penultimate layer.
The parameters of two neural networks (for skeleton and depth) are loaded from the previously trained individual module...The results for the early fusion system are reported in Tab.~\ref{Table_score_fusion}. The fusion network is initialised by the pre-trained model and stacked with one hidden layer with 2024 hidden unites. We fine-tune the network and stop the training when the validation error rate stops decreasing ($\sim$15 epochs)...However, we can see from Tab.~\ref{Table_score_fusion} that the early fusion system does not outperform the late fusion system. The result is counter-intuitive because we expect that the early fusion  multimodal feature learning would extract semantically meaningful shared representations, outperforming individual modalities, and the early fusion schemes efficacy against the traditional method of late fusion~\cite{wu2014multimodal}. One possible explanation could be that one individual module has dominant effect on the learning process so as to screw the network towards learning that specific module. The mean activations of the neurons for each module in Fig.~\ref{fig:fusion} indicate the aforementioned conjecture."}

\vspace{5mm}

\rev{State of the art in the experimental section should be mentioned more consistently. For fair comparison, first three lines in Table 3 should be:
[39] Deep learning (step 4): skeleton 0.7891, video 0.7990, fusion 0.8449 [39] Deep learning (multiscale): skeleton 0.8080, video 0.8096, fusion 0.8488 [40] 3 sets of skeletal features and HoG: skeleton 0.791, fusion 0.8220
Therefore, it shows that both learning-based and feature extraction-based approaches outperform the proposed method on each modality, as well as on a combination of them.
Furthermore, it would be interesting to see how the HMM contributes in the performance in comparison with simple voting based on frame-based predictions.}

\ans{
Thank you for your detailed comments. We have amended the results table accordingly. The less than maximum performance could be due to the less than ideal settings and initialisations of the neural network. Nonetheless, we would like to argue that one major contribution of the paper is using the learning method for feature extraction and the utilisation of HMM for simultaneous gesture segmentation and recognition. We also present some brief analysis of why the fusion network didn't achieve expected performance gain and hope the experimental analysis could cast some light on the future research directions of the related problems.}

\vspace{5mm}

\rev{
Visualization of the filter banks (section 3.3.4) in its current state is unnecessary as it does not provide any interesting insights on the interpretation of the learned features. Instead, the poorly formed filters rather indicate undertraining, or lack of training data given the model complexity, or suboptimality of training procedure.}

\ans{
We  include the response maps after filtering for both body and hand parts. We observe some interesting properties from the visualisation of the filter banks as in~\cite{socher2012convolutional} that ``Depth images have sharper edges and generally are smoother than the grayscale filters, though the distinctions are less obvious compared with the body versus hand filters."}



% -----------------------------------------------------------------------------------
\newpage
{\LARGE \noindent Response to Reviewer 3}\newline

We thank the reviewer for his time and insightful comments.
%
Below, we provide our specific answers to his comments and to the unclear points that were raised,
and what we have done to clarify the paper and take comments into account.
%
Note that in our answer, all references (Equations, Figures)
refer to the new version unless stated otherwise.


\qquad

\rev{The paper proposes the fusion of the output from a Gaussian-Bernoulli Deep Belief network operating on skeletal features and the output of a Convolutional Neural Network operating on RGBD data to perform gesture segmentation and recognition. 
The paper advances the field of gesture recognition by using both data sources and deep learning architectures within a Hidden Markov Model chain. 
The results are improved compared to using either architecture independently.

In general, I would be more excited if shared representations were learned from the skeleton and the RGB data, as done in multimodal deep learning. 
This is left for future work. The paper might not have enough new material to warrant a PAMI publication wrt to the previous conference versions. 
I also find that it is not that well written for a journal paper (see below).  
On the positive side, the CNN and DBN are technically sound and the results from their fusion are interesting.

One would expect that the journal version of the paper would be more self-contained and easier to follow than the conference versions, but here I observe the opposite trend. For example, the older conference version ~\cite{diwucvpr14} explains the intuition behind the higher level representation of the skeleton features, but the journal version does not. The conference paper explains how the coordinate frames are built for the features, while this paper skips this part. The conference paper explains the datasets and visualizes the Viterbi paths better.
}

\ans{Thank you for your careful and positive review. 
We agree that in this journal version of the paper, some self-contained information had been omitted from the conference paper, and that there were unclear points.
%
We have significantly reworked the paper and improved its structure to both improve clarity and account for the reviewer's comment:
\begin{compactitem}
\item a related works section has been added;
\item Section III now describe the method overview, both in terms of HMM modeling (Section III.A and III.B), and in terms of Neural network modeling (Section III.C), 
including with new figures;
\item more intuition about the temporal modeling (Section III.A);
\item more motivation and intuition behind the use of learned higher level representation and the advantages offered by 
DBN models for emission probability modeling over the use of GMMs, including the crucial importance of the Gaussian-Bernouilli Restricted Bolzman Machines 
for pretraining and  initializing the deep belief network.
\item the experimental part includes a proper description of the dataset, experimental protocol (including performance measures), and more result analysis.
\item finally, an early fusion scheme that learns a shared representation has also been implemented and evaluated. Its description is provided in Section IV.D. 
However, to the contrary of what we had expected, it did not perform better than the late fusion scheme.
\end{compactitem}
We hope and believe that this revision  is now much more self-contained and will better suit the quality standard of a journal paper.
}


\rev{Section 2 does not help much the reader understand the formulation. For example:
 "At each time step, we have one observed random variable $X_t$: explain what these variables represent early (raw skeletan input / \RGBD)  we have an unobserved variable $H_t$: describe at a high level the information that the unobserved variables capture, mention examples}

\ans{
We have considerably reworked Section III to provide an overview of the method  along with intuition  and motivations. 
Please check the new version. More specific parts of this section addressing your more specific comments are provided below.\\[2mm]
%
Regarding the variables:

\PM{
A continuous-observation HMM  is adopted for modelling higher level temporal relationships.
At each time step $t$, we have one observed random variable $\observation_t$
composed of the skeleton input $\observationSK_t$ and RGB-D input images $\observationRGBD_t$
as shown in the graphical representation in Fig.~\ref{fig:GM}.
%
 The hidden state variable $\hiddenstate_t$ takes on values in a finite set  \finiteset composed of \numberhiddenstate states related to
the different gestures. 
 The intuition motivating the HMM model is that a gesture is composed of a sequence of poses where the relative duration of each pose may vary. 
This variance is captured by allowing flexible forward transitions within a Markov chain.
In practice,  $\hiddenstate_t$ can be interpreted as being in a particular phase  of a gesture \gesturea{}.
}

\vspace*{2mm}

\noindent Or related to more concrete example (Section III.B abut Markov state diagram):

\PM{
For each given gesture $a \in \mathcal{A}$, a set of states $\mathcal{H}_a$ is introduced to defined a Markov model of that gesture. For example, for action sequence ``tennis serving", the action sequence can implictly be dissected into $\hiddenstatenode_{a_1}, \hiddenstatenode_{a_2}, \hiddenstatenode_{a_3}$ as: 1) raising one arm 2) raising the racket 3) hitting the ball.
}

}


\rev{The related work section is out of place after the technical sections and before the experiments.}

\ans{We have now written a proper related work section after the introduction section.}

\rev{There is no point writing a loop for m=1:2 in Algorithm 1 and 2.}

\ans{The Algorithm description has been removed from the paper. 
We have priviledged a more structured and more textual description of the method, and thus needed to remove the Algorithms for space reason. 
Nevertheless, we believe that given the method overview in Section III, and the more detailed elements in Section IV, the method steps should be fairly easily 
understandable.
}


\rev{"the number of states ... is chosen as 5": any intuition here?}

\ans{Thank you for the comment and this is a very good observation.  The main intuition behind this number was that a gesture
is often composed of 5 phases: 1) an onset; 2)  arm motions to reach 3) a more static pose (often characterized by a distinct hand posture); 
and a4)  motion back to 5) stop in  the rest pose.
%
However, we agree that  this number might not be optimal, and that different gestures could have different number of states.
Also, from a more heuristic point of view, note that we had performed experiments with 10 states per class, and that it performed similarly.
%

To account for the reviewer's comment we have updated the text as follows:

\PM{Note that intuitively, 5 states represents a good granularity as 
most gestures in the Clalearn are composed of 5 phases: an onset, followed by arm motions to reach a more static pose 
(often characterized by a distinct hand posture), and the motion back to the rest place.
%
In the future, optimal section of this number\footnote{Experiments with 10 states led to similar performance.} 
and of different number of states per gesture could be investigated.
}

}


\rev{"10 frames are assigned to hidden state ...": why 10?}

\ans{Thank you for the careful observation. This is actually a written error. The corrected tet reads now:

\PM{To do so, a force alignment is used which means that if the $i^{th}$ sequence is a gesture \gesturea{}, then the first $\lfloor \frac{T_i}{5} \rfloor$ frames are assigned to state $\hiddenstatenode_\gesturea^1$ (the first state of gesture \gesturea{}),
the following $\lfloor \frac{T_i}{5} \rfloor$ frames are assigned to $\hiddenstatenode_\gesturea^2$, and so forth.
}
}

\rev{it is hard to interpret the learned features on Figure 8. There is no intuition what the depth filters capture.}

\ans{Because our filter size is $5\times5$ (smaller filter sizes tend to generalize better, ~\cite{simonyan2014very} used $3\times3$ convolution filters), 
their interpretation is indeed some hard, although one can notice that depth filters capture smooth depth transitions, and the combined image and depth filter 
(see architecture description in Section IV.C.2) can represent two types of information: segmentation, and edge. 
We have modified the text to provide better insight as follows:

\PM{
The convolutional filter weights of the first layer are depicted in Fig.~\ref{3dcnn_filters}.
The unique characteristics from the kernels are clearly visible: as hand input images (RGB and depth) have larger homogenous areas 
than the body inputs, the resulting filters are smoother than their body counterpart.
In addition, while being smoother overall than the grayscale filters, depth filters exhibit stronger edges,
as also reported in~\cite{socher2012convolutional}.
%
Finally, by looking at the joint depth-image response maps, we can notice that some filters better capture segmentation like information, 
while other are more edge oriented.
}
}


\rev{Citations that could be added in the context of deep learning from RGBD data: "Convolutional-Recursive Deep Learning for 3D Object Classification", 
Socher et al., NIPS 2012, and 
"Learning Rich Features from RGB-D Images for Object Detection and Segmentation", Gupta et al., ECCV 2014.
}

\ans{
Thank you for the suggested citations. We find those work interesting, e.g.  for Gupta \emph{et al.}~\cite{gupta2014learning},
as it shows that CNN do not necessarily need to be trained from the raw images, and some handcrafted features may better 
help the network to learn more meaningful, higher level representations. We have added these references in the paper in the following way:

\PM{
However, the advent of  Kinect-like sensors has put more emphasis on RGB-D data for gesture recognition, but not only.
For instance, the benefits of deep Learning using RGB-D data have been explored for object detection or classification tasks.
%
Socher \emph{et al.}~\cite{socher2012convolutional} proposed a single convolutional neural net layer for each modality as inputs to multiple, fixed-tree RNNs in order to compose higher order features for 3D object classification. The single convolutional neural net layer provides useful translational invariance of low level features such as edges and allows parts of an object to be deformable to some extent.
% The pooled filter responses are then give to a recursive neural network to learn compositional features and part interactions.
To address object detection, Gupta \emph{et al.}~\cite{gupta2014learning} proposed a geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity.
%
}
}


\rev{Another related work is the "Multimodal Deep Learning" by Ngiam et al., ICML 11. I would also like to see some discussion wrt "Hidden Conditional Random Fields for Gesture Recognition", Wang et al, CVPR 2006}

\ans{
Thank you for suggesting very relevant works. We came across both papers. "Multimodal Deep Learning" essentially is the prototype for an early fusion model. 
%
Regarding Wang \emph{et al.}~\cite{wang2006hidden}), the similarity with our proposed method is that both methods use a/several hidden layer 
for learning higher level representations. 
However, authors in Wang \emph{et al.}~\cite{wang2006hidden})  observed that one hidden layer is limited for learning a larger class corpus. 
In our case, we believe that higher level representation learning with more layers, which is an essential part of our paper, 
is very important for gesture classification. 
%Moreover, the partition function of CRF makes the discriminative training more difficult to train.  
%
Recent advancement in multi-layer feature learning and pre-training for DBN renders our proposed method more meaningful. We have included the references as follows:\\[-3mm]

\noindent For Ngiam et al, as it does not relate to gesture recognition, we have cited this method in the introduction.

\PM{
Multimodal deep learning technique were also investigated \cite{Ngiam2011multimodal} to learn cross-modality representation, 
for instance in the context of audio-visual speech recognition.
}

\noindent However, as their method is very similar to the early fusion scheme we have now implemented, we have added the following in Section IV.D.2:

\PM{
Note that this is very similar to the approach proposed in \cite{Ngiam2011multimodal}
for audio-visual speech recognition. 
%
An important difference is that in \cite{Ngiam2011multimodal}, the same stacked RBMs/\DBN architecture was used 
to represent both modalities before fusion, whereas in our case, a stacked RBMs/\DBN and a \ThreeDCNN are used.
%
Also, \cite{Ngiam2011multimodal} proposed the use of a multimodal autoencoder to handle predictions when potentially 
only one modality migth be present, a point that we do not address.
}

\vspace*{2mm}

\noindent For Wang et al:

\PM{
Wang \emph{et al.}~\cite{wang2006hidden} introduced a more elaborated discriminative hidden-state approach for the recognition of human gestures.
However, relying on only one layer of hidden states,
their model alone might not  be powerful enough to learn a higher level representation of the data and take advantage of very large corpus. 
In this paper, we adopt a different approach by focusing on deep feature learning within a temporal model.
}

}



% -----------------------------------------------------------------------------------
\newpage
{\LARGE \noindent Response to Reviewer 4}\newline


We thank the reviewer for his time and insightful comments.
%
Below, we provide our specific answers to his comments and to the unclear points that were raised,
and what we have done to clarify the paper and take comments into account.
%
Note that in our answer, all references (Equations, Figures)
refer to the new version unless stated otherwise.


\rev{
The paper purports 3 contributions.
(1) the authors use deep learning to estimate emission probabilities for a HMM predicting gesture.

(2) They use a 3d convolutional network. While the introduction makes it sound like this is for multiple-channels (e.g. RGB + Depth), sec. 3.3.2 makes it clear the 3rd dimension is time as the model processes 4 frame sub-sequences. I think, Fig. 6 could be clearer. 

(3) Emission probability models are trained for both the skeletal data and depth data. They are then averaged (weighted) and used in an HMM. 

 Overall, I am convinced this paper solves the problem of gesture recognition with a novel combination of techniques. However, I am not convinced (1) any of the technical techniques themselves are particularly novel nor (2) that the chosen combination is the right one. Finally, (3) the results aren't particularly impressive (only matching state-of-the-art). Moreover, I have technical/philosophical objections which I'll elaborate on in the comments. 

Learning to model HMM emission and transition parameters is an old idea (going back decades, 
 to at least the well known Baum-Welch algorithm) and 3D convolutional networks for video were explored by [11]. 

Using RGB-D with deep learning is a common idea, explored by many concurrent works e.g. [A,B,C]. 
 [A] Dosovitskiy, A., Springenberg, J. T., Riedmiller, M.,  Brox, T. (2014). Discriminative Unsupervised Feature Learning with Convolutional Neural Networks. 
 Arxiv Preprint arXiv: 1-13.
 [B] Gupta, S., Girshick, R., Arbeláez, P.,  Malik, J. (2014). Learning Rich Features from RGB-D Images for Object Detection and Segmentation. arXiv Preprint arXiv:1407.5736, 1–16. doi:10.1007/978-3-319-10584-0 23
 [C] Socher, R., Huval, B., Bhat, B., Manning, C. D.,  Ng, A. Y. (2012). Convolutional-Recursive Deep Learning for 3D Object Classification. Advances in Neural Information Processing Systems 25, (i), 665-673.
}

\ans{
answer =

\mycomline{we agree, HMM is not novel - we need to add one or two reference and some text about that (Nam)}

\mycomline{clarify 3rd dimension in intro (JMO), + figure 6 (now xx) should be clear}



}



\rev{Late fusion: my greatest technical concern is that two deep models are trained and then combined with a weighted average: s = a * s1 + (1-a)*s2 where a is chosen by cross-validation. Instead, the authors could combine the two models by creating a new top-level perceptron layer which takes the two models as input. Then this whole structure could be trained jointly with back-propagation. I'd expect results to be (1)at least as good and (2) more philosophically unified. }

\ans{We agree with your insightful observation, and actually believe that this should improve the system.
We thus implemented such a scheme 
 and continue experimenting with such an intermediate fusion fusion scheme in this revision. 
However, this approach did not improve the results over the late fusion scheme, providing similar results.
%Our previous paper~\cite{wu2014multimodal} utilized the early fusion scheme for audio and skeleton modules for action recognition. 
%We followed that strategy and perform the early fusion scheme using penultimate layer in Section\ref{early_fusion}.

To account for this new model, the text was updated as follows in the model description and the result analysis. 
In Section IV.D.2, describing the Intermediate fusion:

\PM{
As an alternative to the late fusion scheme, we can take advantage of the high-level representation implicitly learned by each module
(and represented by the \highSK and \highRGBD nodes of the penultimate layer of the respective networks, before the softmax)
to fuse the modality in an intermediate fashion by concatenating these two layers in one layer of 2024 hidden unites
and learning a cross-modality emission probability predictive network.
%
Note that this is very similar in spirit to the approach proposed in \cite{Ngiam2011multimodal}
for audio-visual speech recognition.
%
An important difference is that in \cite{Ngiam2011multimodal}, the same stacked RBMs/\DBN architecture was used
to represent both modalities before fusion, whereas in our case, a stacked RBMs/\DBN and a \ThreeDCNN are used.
%
Also, \cite{Ngiam2011multimodal} proposed the use of a multimodal autoencoder to handle predictions when potentially
only one modality migth be present, a point that we do not address.

The resulting architecture is trained by first initializing the weights of the deeper layers from the previously trained module,
and then jointly fine tuning the whole network (including learning the last layer parameters)
and stop the training when the validation error rate stops decreasing ($\sim$15 epochs).
%
We argue that using the ``pre-trained" parameters is important due to the heterogeneity of the inputs of the system,
and that the joint training should adjust parameters to handle  this heterogeneity and produce the final estimates.
}

A specific section in the analysis is devoted to the result analysis, which reads:

\PM{
\mypartitle{Late vs. Intermediate  fusion.}
%
The results in Tab.~\ref{tab:jaccardperformance} and~\ref{tab:eventperformance} show that the early fusion system
improved individual modalities, but without outperforming the late fusion system.
%
The result is counter-intuitive, as we would have expected the cross-modality learning in the early fusion scheme
to result in better emission probability predictions, as compared to the simple score fusion in the late system.
%
One possible explanation is that the independance assumption of the late scheme better preserves both
the complementarity and redundancy of the different modalities, properties which are important for fusion.
%
Another related explanation is that in the early fusion learning process,
one modality may dominate and  skew the network towards  learning that specific module and
lowering the importance of the other one.
%
The large difference between the mean activations of the  skeleton module neurons which are predominantly larger than those of the
RGB-D ConvNet's (0.57 \emph{vs.} 0.056) can be an indicator of such a bias during the multimodal fine-tuning phase and
support this conjecture, even if these mean activations are not directly comparable
due to the neuron heterogeneity (the skeleton DBN has logistic units whereas the 3DCNN ConvNet has relu units).
%
Note that such heterogeneity was not present when fusing modalities  in \cite{Ngiam2011multimodal},
where better registration and less spatial registration variability in lip images
allowed to also resort to the same stacked RBMs for the visual modality (rather than \ThreeDCNN)  and the audio one.
%
More investigation on how to handle heteregeneous networks  should be conducted.
}

}


\rev{The analysis is a bit brief. More experiments and ablative analysis could be added. Specifically, can we interpret the failure patterns of the proposed model(s) and prior work? It would be interesting to see statements like [40] fails more often on gestures of X kind because HOG erases Y useful information or [39] does worse for Z because it handles time at an earlier stage of the pipeline. Then, also giving some qualitative examples of these failures.}

\ans{
We agree that there is a lack of experiments analysis, especially the failure patterns and lessons learnt from the experiments. We have included more analysis in the Experiment and Analysis section as follows:
``Examples of overall upper body movements influence on the system performance. Left (score: 0.94) performer almost kept a static upper body whilst performing Italian sign language. Right (score: 0.34) performer moved vehemently when performing the gestures.\ref{fig:bodydynamics}"
}

\vspace{5mm}

\rev{These extra experiments (considering joint training of a combined emission probability model) and qualitative interpretation could significantly affect the paper. Overall, the research is solid but needs significantly more work before publication.
RCNN: Last, it is entirely possible to train a recurrent neural network to perform Viterbi decoding. This may be difficult (requiring more training data) but would make the entire paper fit into a the deep learning framework. I cannot hold this against the authors, but some discussion might help.}

\ans{
We have included more qualitative interpretation of the results.
We agree that a recurrent neural network could potentially replace the Viterbi decoding part to make the system as a more unified end-to-end system. This, however, may be left to the future work.
}

\vspace{5mm}

\rev{
They use a 3d convolutional network. While the introduction makes it sound like this is for multiple-channels (e.g. RGB + Depth), sec. 3.3.2 makes it clear the 3rd dimension is time as the model processes 4 frame sub-sequences. I think, Fig. 6 could be clearer.}

\ans{
Thank you for your detailed observation. Yes, the 3rd dimension of the input network is indeed the time axis. However, RGB and Depth data are treated as the two channels during the input phase. We detailed the description of the Figure as follows.
``The 3rd dimension of the input is time with 4 frames stacked together. The depth and RGB data are stacked (concatenated) together at Input. Hand and body part outputs are concatenated at H7."
}

\vspace{5mm}

\rev{
Using RGB-D with deep learning is a common idea, explored by many concurrent works e.g. [A,B,C].
[A] Dosovitskiy, A., Springenberg, J. T., Riedmiller, M., \& Brox, T. (2014). Discriminative Unsupervised Feature Learning with Convolutional Neural Networks. arXiv Preprint arXiv: 113.
\newline
[B] Gupta, S., Girshick, R., Arbeláez, P., \& Malik, J. (2014). Learning Rich Features from RGB-D Images for Object Detection and Segmentation. arXiv Preprint arXiv:1407.5736, 116. doi:10.1007978-3-319-10584-0-23
\newline
[C] Socher, R., Huval, B., Bhat, B., Manning, C. D., \& Ng, A. Y. (2012). Convolutional-Recursive Deep Learning for 3D Object Classification. Advances in Neural Information Processing Systems}

\ans{
Thank you for suggesting related works.  We find the works in Gupta \emph{et al.}~\cite{gupta2014learning} interesting in a sense that CNN does not necessarily need to train from the raw images, and some handcrafted features may better help the network to learn more meaningful, higher level representations.
We have included the papers using deep learning for RBG-D data with discussions in the related works section:
``Gupta \emph{et al.} proposed a geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. The depth image was represented by three channels: horizontal disparity, height above ground and angle with gravity. This augmented representation allows the CNN to learn strong features than by using disparity (or depth) alone."
``In the field of deep learning from RGBD data, Socher \emph{et al.} proposed a single convolutional neural net layer for each modality as input to multiple, fixed-tree RNNs in order to compose higher order features for 3D object classification. The single convolutional neural net layer provides useful translational invariance of low level features such as edges and allows parts of an object to be deformable to some extent. The pooled filter responses are then given to a recursive neural network to learn compositional features and part interactions."
}



\endinput
