
\section{Related Work}
\label{sec:relatedwork}

Gesture recognition has drawn increasing attention from researchers, primarily due to its growing potential in areas such as robotics, human-computer interaction and user interface design. Different temporal models have been proposed.
Nowozin and Shotton~\cite{nowozin2012action} proposed the notion of ``action points" to serve as natural temporal anchors of simple human actions using a Hidden Markov Model.
Wang \emph{et al.}~\cite{wang2006hidden} introduced a more elaborated discriminative hidden-state approach for the recognition of human gestures.
However, relying on only one layer of hidden states,
their model alone might not  be powerful enough to learn a higher level representation of the data and take advantage of very large corpus. In this paper, we adopt a different approach by focusing on deep feature learning within a temporal model.

There have been a few works exploring deep learning for action recognition in videos. For instance, Ji \emph{et al.}\cite{ji20133d} proposed using 3D convolutional neural network for automated recognition of human actions in surveillance videos. Their model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. To further boost the performance, they proposed regularising the outputs with high-level features and combining the predictions of a variety of different models. Taylor \emph{et al.}~\cite{taylor2010convolutional}  also explored 3D convolutional networks for learning spatio-temporal features for videos. The experiments in~\cite{wu2014deep} show that multiple network averaging works better than a single individual network and larger nets  will generally perform better than smaller nets.
Providing there is enough data, averaging multi-column nets~\cite{ciresan2012multi} applied to
action recognition could also further improve the performance.

However, the advent of  Kinect-like sensors has put more emphasis on RGB-D data for gesture recognition, but not only.
For instance, the benefits of deep learning using RGB-D data have been explored for object detection or classification tasks.
%
Dosovistskiy \emph{et al.}~\cite{DosovitskiySRB14} presented generic feature learning for training a convolutional network using only unlabeled data. In contrast to supervised network training, the resulting feature representation is not class specific and is advantageous on geometric matching problems, outperforming the SIFT descriptor.
Socher \emph{et al.}~\cite{socher2012convolutional} proposed a single convolutional neural net layer for each modality as inputs to multiple, fixed-tree RNNs in order to compose higher order features for 3D object classification. The single convolutional neural net layer provides useful translational invariance of low level features such as edges and allows parts of an object to be deformable to some extent.
% The pooled filter responses are then give to a recursive neural network to learn compositional features and part interactions.
To address object detection, Gupta \emph{et al.}~\cite{gupta2014learning} proposed a geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity.
%The depth image was represented by three channels: horizontal disparity, height above ground and angle with gravity.
This augmented representation allows CNN to learn stronger features than when using disparity (or depth) alone.

Recently, the gesture recognition domain itself has been stimulated by the collection of large public corpus.
In particular, the ChaLearn 2013 competition was populated by the exploitation of the HMM models:
Nandakumar \emph{et al.}~\cite{nandakumar2013multi} applied the MFCC+HMM paradigm for audio input, Space-Time-Interest-Point (STIP), Bag-of-Words (BoW) and SVM pipeline for RGB videos and a covariance descriptor with SVM for skeleton module prediction. The 1st ranked team by Wu \emph{et al.}~\cite{wu2013fusing} used HMM model as audio feature classifier and Dynamic Time Warping as the classifier for skeleton feature.  The Recurrent Neural Network (RNN) was deployed in~\cite{neverova2013multi} to model large-scale temporal dependencies, data fusion and gesture classification.  Interestingly, the system in ~\cite{neverova2013multi} decomposed the gestures into a large-scale body motion and local subtle movements.

The ChaLearn LAP \cite{chalearnLAP} gesture spotting challenge has collected around 14,000 gestures drawn from a vocabulary of 20 Italian sign gesture categories. The emphasis is on multi-modal automatic learning gestures performed by several different users, with the aim of performing user independent continuous gesture spotting.
Some of the top winning methods in the ChaLearn LAP gesture spotting challenge require a set of complicated handcrafted features for either skeletal input, RGB-D input, or both.
For instance, Neveroa \emph{et al.}~\cite{neverova2014multi} proposed a pose descriptor consisting of 7 logical subsets for skeleton features while Monnier \emph{et al.}~\cite{Monnier2014multi} proposed to use 4 types of features for skeleton features (normalised joint positions; joint quaternion angles; Euclidean distances between specific joints; and directed distances between pairs of joints, based on the features proposed by Yao \emph{et al.}~\cite{yao2011does}) and histograms of oriented gradients (HOG) descriptor for RGB-D images around hand regions.
In~\cite{Peng2014multi}, the state-of-the-art dense trajectory~\cite{wang2013dense} handcrafted features are adopted for the RGB module.

There is a gradual trend to learn the features for gesture recognition in videos.
%
For instance, the recent methods in \cite{wu2014deep,lio2014deep} focused on single modalities,
used deep networks to learn representations from skeleton data ~\cite{wu2014deep} or from RGB-D data ~\cite{lio2014deep}.
%
Neveroa \emph{et al.}~\cite{neverova2014multi} presents a multi-scale and multimodal deep network for gesture detection and localisation.
Key to their technique is a training strategy that exploits i) careful initialisation of individual modalities and ii) gradual fusion of modalities from the strongest to weakest cross-modality structure.
%
One major difference compared to what we propose is the treatment of the time factor:
rather than using a temporal model, they used frames within a fixed interval as the input of their neural networks
for the prediction of the final gesture class, an approach that
requires require to train several multi-scale temporal networks  to cope with gestures performed at different speeds.
%
Furthermore, their skeleton features used in their network are sets of ad-hoc hand crafted features, rather than being
learned from raw data.


\endinput
