
\section{Related Work}
\label{sec:relatedwork}

Gesture recognition has drawn increasing attention from researchers, primarily due to its growing potential in areas such as robotics, human-computer interaction and user interface design. Different temporal models have been proposed.
Nowozin and Shotton~\cite{nowozin2012action} proposed the notion of ``action points" to serve as natural temporal anchors of simple human actions using a Hidden Markov Model.
Wang \emph{et al.}~\cite{wang2006hidden} introduced a more elaborated discriminative hidden-state approach for the recognition of human gestures.
However, relying on only one layer of hidden states,
their model alone might not  be powerful enough to learn a higher level representation of the data and take advantage of very large corpora. In this paper, we adopt a different approach by focusing on deep feature learning within a temporal model.

There have been a few works exploring deep learning for action recognition in videos. For instance, Ji \emph{et al.}\cite{ji20133d} proposed using 3D convolutional neural network for automated recognition of human actions in surveillance videos. Their model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. To further boost the performance, they proposed regularising the outputs with high-level features and combining the predictions of a variety of different models. Taylor \emph{et al.}~\cite{taylor2010convolutional}  also explored 3D convolutional networks for learning spatio-temporal features for videos. The experiments in~\cite{wu2014deep} show that multiple network averaging works better than a single individual network and larger nets  will generally perform better than smaller nets.
Providing there is enough data, averaging multi-column nets~\cite{ciresan2012multi} applied to
action recognition could also further improve the performance.

The introduction of  Kinect-like sensors has put more emphasis on RGB-D data for gesture recognition but has also influenced other video-based recognition tasks.
For example, the benefits of deep learning using RGB-D data have been explored for object detection or classification tasks.
%
Dosovistskiy \emph{et al.}~\cite{DosovitskiySRB14} presented generic feature learning for training a convolutional network using only unlabelled data. In contrast to supervised network training, the resulting feature representation is not class specific and is advantageous on geometric matching problems, outperforming the SIFT descriptor.
Socher \emph{et al.}~\cite{socher2012convolutional} proposed a single convolutional neural net layer for each modality as inputs to multiple, fixed-tree RNNs in order to compose higher order features for 3D object classification. The single convolutional neural net layer provides useful translational invariance of low level features such as edges and allows parts of an object to be deformable to some extent.
% The pooled filter responses are then give to a recursive neural network to learn compositional features and part interactions.
To address object detection, Gupta \emph{et al.}~\cite{gupta2014learning} proposed a geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity.
%The depth image was represented by three channels: horizontal disparity, height above ground and angle with gravity.
This augmented representation allows CNN to learn stronger features than when using disparity (or depth) alone.

Recently, the gesture recognition domain has been stimulated by the collection and publication of large corpora.
One such corpus was made available for the ChaLearn 2013 competition, in which HMM models were used by many participants:
Nandakumar \emph{et al.}~\cite{nandakumar2013multi} applied the MFCC+HMM paradigm for audio input, Space-Time-Interest-Point (STIP), Bag-of-Words (BoW) and SVM pipeline for RGB videos and a covariance descriptor with SVM for skeleton module prediction. The 1st ranked team, Wu \emph{et al.}~\cite{wu2013fusing}, used and HMM model as audio feature classifier and Dynamic Time Warping as the classifier for skeleton features.  A Recurrent Neural Network (RNN) was utilised in~\cite{neverova2013multi} to model large-scale temporal dependencies, for data fusion and for the final gesture classification.  Interestingly, the system in ~\cite{neverova2013multi} decomposed the gestures into a large-scale body motion and local subtle movements.

As a follow up, the ChaLearn LAP \cite{chalearnLAP} gesture spotting challenge has collected around 14,000 gestures drawn from a vocabulary of 20 Italian gestures. The emphasis in this dataset is on user-independent online classification of gestures. Several of the top winning methods in the ChaLearn LAP gesture spotting challenge require a set of complicated hand-crafted features for either skeletal input, RGB-D input, or both.
For instance, Neveroa \emph{et al.}~\cite{neverova2014multi} proposed a pose descriptor consisting of 7 subsets for skeleton features. Monnier \emph{et al.}~\cite{Monnier2014multi} proposed to use 4 types of features for the skeleton (normalised joint positions; joint quaternion angles; Euclidean distances between specific joints; and directed distances between pairs of joints). This was based on the features proposed by Yao \emph{et al.}~\cite{yao2011does}) Additionally, he also used a histograms of oriented gradients (HOG) descriptor for RGB-D images around the hand regions.
In~\cite{Peng2014multi}, hand-crafted features based on dense trajectories~\cite{wang2013dense} are adopted for the RGB module. 

There is however also the trend to learn the features, in contrast to engineering them, for gesture recognition in videos.
%
For instance, the recent methods in \cite{wu2014deep,lio2014deep} focused on single modalities,
used deep networks to learn representations from skeleton data ~\cite{wu2014deep} or from RGB-D data ~\cite{lio2014deep}.
%
Neveroa \emph{et al.}~\cite{neverova2014multi} presents a multi-scale and multimodal deep network for gesture detection and localisation.
Key to their technique is a training strategy that exploits i) careful initialisation of the sub-components of individual modalities and ii) gradual fusion of modalities from the strongest to weakest cross-modality structure.
%
One major difference compared to what we propose is the treatment time:
rather than using a temporal model, they used frames within a fixed interval as the input of their neural networks. This approach requires the training of several multi-scale temporal networks  to cope with gestures performed at different speeds.
%
Furthermore, the skeleton features they used are hand-crafter and whereas our features are learned from data.

\endinput
