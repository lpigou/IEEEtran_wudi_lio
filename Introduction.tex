

\section{Introduction}
\label{sec:introduction}

\IEEEPARstart{I}{n} recent years, human action recognition has drawn increasing attention of researchers, primarily due to its potential in areas such as video surveillance, robotics, human-computer interaction, user interface design, and multimedia video retrieval.

Previous works on video-based action recognition~\cite{liuli,xiantong,diwu2} focused mainly on adapting hand-crafted features. % and low-level hand-designed features.
These methods usually have two stages: an optional feature detection stage followed by a feature description stage. Well-known feature detection methods are Harris3D~\cite{laptev2005space}, Cuboids~\cite{dollar2005behavior} and Hessian3D~\cite{hession3d}. For descriptors, popular methods are Cuboids~\cite{scovanner20073}, HOG/HOF~\cite{laptev2005space}, HOG3D~\cite{klaser:inria-00514853} and Extended SURF~\cite{hession3d}.
In the recent work of Wang \textit{et al.}~\cite{wang2013dense}, dense trajectories with improved motion based descriptors other hand-crafted features and achieved state-of-the-art results on a variety of datasets.
Based on the current trends, challenges and interests within the action recognition community, it is to be expected that many successes will follow. However, the very high-dimensional and dense trajectory features usually require the use of advanced dimensionality reduction methods to make them computationally feasible.

Furthermore, as discussed in the evaluation paper of Wang \emph{et al.}~\cite{wang2009evaluation}, the best performing feature descriptor is dataset dependent and no universal hand-engineered feature that outperforming all others exists. This clearly indicates that the ability to learn dataset specific feature extractors can be highly beneficial and further the current state of the art.
%In the evaluation paper of Wang \emph{et al.}~\cite{wang2009evaluation}, one interesting finding is that there is no universally best hand-engineered feature for all datasets, suggesting that learning features directly from the dataset itself may be more advantageous.
For this reason, even though hand-crafted features have dominated image recognition in previous years, there has been a growing interest in learning low-level and mid-level features, either in supervised, unsupervised, or semi-supervised settings~\cite{taylor2010convolutional,le2011learning,baccouche2005spatio}.
%Albeit the dominant methodology for visual recognition from images and videos relies on hand-crafted features, there has been a growing interest in methods that learn low-level and mid-level features, either in supervised, unsupervised, or semi-supervised settings~\cite{taylor2010convolutional,le2011learning,baccouche2005spatio}.


Since the recent resurgence of neural networks, which was invoked by Hinton and others~\cite{hinton2006fast},
deep neural architectures have become an effective approach for extracting high-level features from data.
In the last few years Deep artificial neural networks have won numerous contests in pattern recognition and representation learning.
Schmidhuber~\cite{schmidhuber2014deep} compiled a historical survey compactly summarizing relevant works with more than 850 entries of credited papers.
From this overview we see that these models have been successfully applied to a plethora of different domains: the GPU-based cuda-convnet implementation~\cite{krizhevsky2012imagenet} , also known as AlexNet, classifies 1.2 million high-resolution images into 1000 different classes; multi-column deep neural networks~\cite{ciresan2012multi} achieve near-human performance on the handwritten digits and traffic signs recognition benchmarks; 3D convolutional neural networks~\cite{3dcnn}~\cite{ji20133d} recognize human actions in surveillance videos; deep belief networks combined with hidden Markov models~\cite{mohamed2012acoustic}~\cite{diwucvpr14} for acoustic and skeletal joints modelling outperform the decade-dominating paradigm of Gaussian mixture models in conjunction with hidden Markov models.
%
Multimodal deep learning techniques were also investigated \cite{Ngiam2011multimodal} to learn cross-modality representation,
for instance in the context of audio-visual speech recognition.
%
And recently, Baidu research proposed the DeepSpeech system~\cite{hannun2014deepspeech} that combines a well-optimised recurrent neural network (RNN) training system, achieving the lowest error rate on a noisy speech dataset. Across the aforementioned research fields, deep architectures have shown great capacity to discover and extract higher level relevant features.

However, direct and unconstrained learning of these complex models remains non-trivial, since the amount of  training data required increases drastically with the complexity of the prediction model. % This is saying the same again should not be here. Does not add any information!!! (ii) training highly complex models with very general learning algorithms is extremely difficult.
It is therefore a common practice to restrain the complexity of the model. This is generally done by operating on small patches to reduce the input dimension and diversity~\cite{baccouche2005spatio}, or by training the model in an unsupervised manner~\cite{le2011learning} such that more (unlabelled) data can be used, or by forcing the model parameters to be identical for different input locations (as in convolutional neural networks~\cite{krizhevsky2012imagenet,ciresan2012multi,3dcnn}).


Thanks to the immense popularity of the Microsoft Kinect~\cite{shotton2011real}~\cite{lingshao2}, there has been a surge in interest in developing methods for human gesture and action recognition from 3D skeletal data and depth images.
A number of new datasets~\cite{ICMI,fothergill2012instructing,guyon2012chalearn,wang2012mining} have provided researchers with the opportunity to design novel representations and algorithms, and test them on a much larger number of sequences.
While gesture recognition based on 3D joint positions may seem trivial, it is actually not. This is due to several factors. First, there is the high dimensionality of the input and the huge variability with which the poses and movements are made.
A second aspect that further complicates the recognition is the segmentation of different gestures. In practice segmentation is as important as the recognition, but it is an often neglected aspect of the current action recognition research, in which it is often assumed that  pre-segmented sequences are available~\cite{laptev2005space}~\cite{marszalek09}~\cite{Kuehne11}.

In this paper we aim to address these issues by proposing a data driven system. We focus on continuous acyclic video sequence labelling, \emph{i.e.} video sequences that are non-repetitive as opposed to longer repetitive activities, \textit{e.g.} jogging, walking and running. By integrating deep neural networks within an HMM temporal framework, we can jointly perform online segmentation and recognition of this continuous stream of gestures. The proposed framework is inspired by the discriminant HMM, which embedded multi-layer perceptron inside an HMM, that was used for continuous speech recognition~\cite{renals1994connectionist}~\cite{bourlard1994connectionist}.
This manuscript is an extension of the works of~\cite{diwucvpr14},~\cite{wu2014deep} and~\cite{lio2014deep}.
The key contributions can be summarized as follows:
\begin{itemize}
\item A Gaussian-Bernoulli Deep Belief Network is proposed to extract high-level skeletal joint features and the learned representation is used to estimate the emission probability needed to infer gesture sequences;
\item A 3D Convolutional Neural Network  is proposed to extract features from 2D multiple channel inputs like  depth and RGB images stacked along the 1D temporal domain;
\item Intermediate and late fusion strategies are investigated in combination with the temporal modelling. The results of both mechanisms show that multiple-channel fusions outperform individual modules;
\item The difference of mean activations in intermediate fusion due to different activation functions is analyzed which is a contribution itself so as to spur further investigation to effectively fuse multi-model, various activations.
\end{itemize}

The remainder of this paper is organised as follows. Section II reviews related work on gesture recognition using temporal models and recent deep learning work on RGB-D data. Section III introduces the formulation of our Deep Dynamic Neural Network model and the intuition behind the high level feature extraction. Section IV details the model implementation. Section V presents the experimental analysis and Section VI concludes the paper with discussions related to future work.


\endinput
