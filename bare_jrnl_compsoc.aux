\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{chalearnLAP}
\citation{chalearnLAP}
\citation{chalearnLAP}
\citation{liuli,xiantong,diwu2}
\citation{laptev2005space}
\citation{dollar2005behavior}
\citation{hession3d}
\citation{scovanner20073}
\citation{laptev2005space}
\citation{klaser:inria-00514853}
\citation{hession3d}
\citation{wang2013dense}
\citation{tzhou,cxu}
\citation{wang2009evaluation}
\citation{taylor2010convolutional,le2011learning,baccouche2005spatio}
\citation{hinton2006fast}
\citation{schmidhuber2014deep}
\citation{krizhevsky2012imagenet}
\citation{ciresan2012multi}
\citation{3dcnn,ji20133d}
\citation{mohamed2012acoustic,diwucvpr14}
\citation{baccouche2005spatio}
\citation{le2011learning}
\citation{krizhevsky2012imagenet,ciresan2012multi,3dcnn}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:introduction}{{1}{1}{Introduction\relax }{section.1}{}}
\citation{shotton2011real,lingshao2}
\citation{ICMI,fothergill2012instructing,guyon2012chalearn,wang2012mining}
\citation{ICMI}
\citation{mohamed2012acoustic}
\citation{diwucvpr14}
\citation{hinton2006fast}
\citation{6751269}
\citation{nowozin2012action}
\@writefile{toc}{\contentsline {section}{\numberline {2}Experiments and Analysis}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Chalearn LAP Dataset \& Evaluation Metrics}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Model Architecture: Deep Dynamic Neural Networks}{2}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  {\relax \fontsize  {8bp}{9bp}\selectfont  Per-action model: a forward-linked chain. Inputs (skeletal features or depth image features) are first passed through Deep Neural Nets (Deep Belief Networks for skeletal modality or 3D Convolutional Neural Networks for depth modality) to extract high level features. The outputs are the emission probabilities of the hidden states. } \relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{GM}{{1}{2}{{\footnotesize Per-action model: a forward-linked chain. Inputs (skeletal features or depth image features) are first passed through Deep Neural Nets (Deep Belief Networks for skeletal modality or 3D Convolutional Neural Networks for depth modality) to extract high level features. The outputs are the emission probabilities of the hidden states. } \relax \relax }{figure.caption.1}{}}
\newlabel{HMM_GM_1}{{2}{2}{Model Architecture: Deep Dynamic Neural Networks\relax }{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\emph  {ES-HMM}: Simultaneous Segmentation and Recognition}{2}{subsection.2.3}}
\citation{bishop2006pattern}
\citation{krizhevsky2012imagenet}
\newlabel{viterbi_GDBN}{{3}{3}{\emph {ES-HMM}: Simultaneous Segmentation and Recognition\relax }{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Experimental Setups}{3}{subsection.2.4}}
\newlabel{hiddenstates}{{2.4}{3}{Experimental Setups\relax }{subsection.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Skeleton Module \& DBN training}{3}{subsection.2.5}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Multimodal Deep Dynamic Networks -- training\relax }}{3}{algocf.1}}
\newlabel{MMDDN_train}{{1}{3}{\emph {ES-HMM}: Simultaneous Segmentation and Recognition\relax }{algocf.1}{}}
\citation{diwucvpr14}
\citation{diwucvpr14}
\citation{fothergill2012instructing}
\citation{mohamed2012acoustic}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  {\relax \fontsize  {8bp}{9bp}\selectfont  State diagram of the \emph  {ES-HMM} model for low-latency action segmentation and recognition. An ergodic states (ES) shows the resting position between action sequence. Each node represents a single frame and each row represents a single action model. The arrows indicate possible transitions between states.} \relax }}{4}{figure.caption.2}}
\newlabel{HMM_ES}{{2}{4}{{\footnotesize State diagram of the \emph {ES-HMM} model for low-latency action segmentation and recognition. An ergodic states (ES) shows the resting position between action sequence. Each node represents a single frame and each row represents a single action model. The arrows indicate possible transitions between states.} \relax \relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  {\relax \fontsize  {8bp}{9bp}\selectfont  Point cloud projection of depth image and the 3D positional features.} \relax }}{4}{figure.caption.4}}
\newlabel{point_cloud}{{3}{4}{{\footnotesize Point cloud projection of depth image and the 3D positional features.} \relax \relax }{figure.caption.4}{}}
\newlabel{sk_features}{{4}{4}{Skeleton Module \& DBN training\relax }{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.5.1}Gaussian Bernoulli Restricted Boltzmann machines}{4}{subsubsection.2.5.1}}
\newlabel{GRBMenergy}{{6}{4}{Gaussian Bernoulli Restricted Boltzmann machines\relax }{equation.2.6}{}}
\citation{torralba2011unbiased}
\citation{ciresan2012multi}
\citation{mnih2013playing}
\citation{krizhevsky2012imagenet}
\citation{wu2012one}
\citation{lewis1995fast}
\citation{opencv_library}
\citation{ji20133d}
\citation{ji20133d}
\citation{krizhevsky2012imagenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Depth 3D Module}{5}{subsection.2.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Preprocessing \& Normalizing: shifting, scaling and resizing.}{5}{subsubsection.2.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  {\relax \fontsize  {8bp}{9bp}\selectfont  Preprocessing.} \relax }}{5}{figure.caption.5}}
\newlabel{original}{{4}{5}{{\footnotesize Preprocessing.} \relax \relax }{figure.caption.5}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Normalization scheme 1: template matching\relax }}{5}{algocf.2}}
\newlabel{normalization_scheme_1}{{2}{5}{Preprocessing \& Normalizing: shifting, scaling and resizing}{algocf.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}3DCNN Architecture \& Details of Learning.}{5}{subsubsection.2.6.2}}
\citation{krizhevsky2012imagenet}
\citation{krizhevsky2012imagenet}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Normalization scheme 2: skeleton normalization\relax }}{6}{algocf.3}}
\newlabel{normalization_scheme_2}{{3}{6}{Preprocessing \& Normalizing: shifting, scaling and resizing}{algocf.3}{}}
\newlabel{Template_Image}{{5a}{6}{{\scriptsize template image}\relax \relax }{figure.caption.8}{}}
\newlabel{sub@Template_Image}{{a}{6}{{\scriptsize template image}\relax \relax }{figure.caption.8}{}}
\newlabel{Test_Image}{{5b}{6}{{\scriptsize test image}\relax \relax }{figure.caption.8}{}}
\newlabel{sub@Test_Image}{{b}{6}{{\scriptsize test image}\relax \relax }{figure.caption.8}{}}
\newlabel{Template_Response}{{5c}{6}{{\scriptsize template response}\relax \relax }{figure.caption.8}{}}
\newlabel{sub@Template_Response}{{c}{6}{{\scriptsize template response}\relax \relax }{figure.caption.8}{}}
\newlabel{shift-resize_image}{{5d}{6}{{\scriptsize shift-resize image}\relax \relax }{figure.caption.8}{}}
\newlabel{sub@shift-resize_image}{{d}{6}{{\scriptsize shift-resize image}\relax \relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces {\relax \fontsize  {8bp}{9bp}\selectfont  Illustration of normalization scheme 1: template matching.}\relax }}{6}{figure.caption.8}}
\newlabel{Normalization scheme 1: template matching}{{5}{6}{{\footnotesize Illustration of normalization scheme 1: template matching.}\relax \relax }{figure.caption.8}{}}
\newlabel{ReLU}{{9}{6}{3DCNN Architecture \& Details of Learning}{equation.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  {\relax \fontsize  {8bp}{9bp}\selectfont  An illustration of the architecture of the 3DCNN architecture.} \relax }}{6}{figure.caption.9}}
\newlabel{3dcnn_architecture}{{6}{6}{{\footnotesize An illustration of the architecture of the 3DCNN architecture.} \relax \relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Looking into the networks–-visualization of filter banks.}{6}{subsubsection.2.6.3}}
\citation{shotton2011real}
\newlabel{frame_error}{{7a}{7}{{\footnotesize Frame based classification error with uncropped input.}\relax \relax }{figure.caption.10}{}}
\newlabel{sub@frame_error}{{a}{7}{{\footnotesize Frame based classification error with uncropped input.}\relax \relax }{figure.caption.10}{}}
\newlabel{frame_error_cropped}{{7b}{7}{{\footnotesize Frame based classification error with cropped input.}\relax \relax }{figure.caption.10}{}}
\newlabel{sub@frame_error_cropped}{{b}{7}{{\footnotesize Frame based classification error with cropped input.}\relax \relax }{figure.caption.10}{}}
\newlabel{cropped}{{7c}{7}{{\footnotesize Cropped images to enhance model's robustness.}\relax \relax }{figure.caption.10}{}}
\newlabel{sub@cropped}{{c}{7}{{\footnotesize Cropped images to enhance model's robustness.}\relax \relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces {\relax \fontsize  {8bp}{9bp}\selectfont  Visualization of the first filters and training statistics for 3DCNN.}\relax }}{7}{figure.caption.10}}
\newlabel{visualization_3dcnn}{{7}{7}{{\footnotesize Visualization of the first filters and training statistics for 3DCNN.}\relax \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Post-Processing}{7}{subsection.2.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces {\relax \fontsize  {8bp}{9bp}\selectfont  Top left: the \emph  {conv1} weights of the 3DCNN learnt with uncropped input; top right: the \emph  {conv1} weights of the 3DCNN learnt with cropped input. It can be seen that filters/weights of the cropped input trained networks are smoother. Bottom: visualization of sample frames after \emph  {conv1} layer (Sample0654, 264-296 frames, sampled every 8 frames). It can be seen that the filters of the first convolutional layer are able to learn both shape pattern(red bounding box) and motion(yellow bounding box). Also note that the high response maps correspond to the most informative part of the body, even though during the training process, all local patches are learned indiscriminately regardless of its location.} \relax }}{7}{figure.caption.11}}
\newlabel{conv1_vis}{{8}{7}{{\footnotesize Top left: the \emph {conv1} weights of the 3DCNN learnt with uncropped input; top right: the \emph {conv1} weights of the 3DCNN learnt with cropped input. It can be seen that filters/weights of the cropped input trained networks are smoother. Bottom: visualization of sample frames after \emph {conv1} layer (Sample0654, 264-296 frames, sampled every 8 frames). It can be seen that the filters of the first convolutional layer are able to learn both shape pattern(red bounding box) and motion(yellow bounding box). Also note that the high response maps correspond to the most informative part of the body, even though during the training process, all local patches are learned indiscriminately regardless of its location.} \relax \relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces {\relax \fontsize  {8bp}{9bp}\selectfont  Illustration of descriptor fusion.} \relax }}{7}{figure.caption.12}}
\newlabel{fusion}{{9}{7}{{\footnotesize Illustration of descriptor fusion.} \relax \relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Score Fusion}{7}{subsection.2.8}}
\bibstyle{IEEEtran}
\bibdata{tPAMI2015}
\bibcite{chalearnLAP}{1}
\bibcite{liuli}{2}
\bibcite{xiantong}{3}
\bibcite{diwu2}{4}
\bibcite{laptev2005space}{5}
\bibcite{dollar2005behavior}{6}
\bibcite{hession3d}{7}
\bibcite{scovanner20073}{8}
\bibcite{klaser:inria-00514853}{9}
\bibcite{wang2013dense}{10}
\bibcite{tzhou}{11}
\bibcite{cxu}{12}
\bibcite{wang2009evaluation}{13}
\bibcite{taylor2010convolutional}{14}
\bibcite{le2011learning}{15}
\bibcite{baccouche2005spatio}{16}
\bibcite{hinton2006fast}{17}
\bibcite{schmidhuber2014deep}{18}
\bibcite{krizhevsky2012imagenet}{19}
\bibcite{ciresan2012multi}{20}
\bibcite{3dcnn}{21}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  {\relax \fontsize  {8bp}{9bp}\selectfont  Comparison of results in terms of Jaccard index between different network structures and various modules. DBDN Net1 corresponds to network structure of $[528,1000,1000,500,201]$ and DBDN Net2 $[528,2000,2000,1000,201]$, DBDN MultiNet is the average of 3 Nets (2 Net1 and 1 Net2 with different initializations). It can be seen that larger net has better performance and multi-column net will further improve the classification rate. Norm1 corresponds to the normalization scheme \ref  {normalization_scheme_1} and Norm2 corresponds to the scheme \ref  {normalization_scheme_2}. } \relax }}{8}{table.caption.13}}
\newlabel{Table_score_fusion}{{1}{8}{{\footnotesize Comparison of results in terms of Jaccard index between different network structures and various modules. DBDN Net1 corresponds to network structure of $[528,1000,1000,500,201]$ and DBDN Net2 $[528,2000,2000,1000,201]$, DBDN MultiNet is the average of 3 Nets (2 Net1 and 1 Net2 with different initializations). It can be seen that larger net has better performance and multi-column net will further improve the classification rate. Norm1 corresponds to the normalization scheme \ref {normalization_scheme_1} and Norm2 corresponds to the scheme \ref {normalization_scheme_2}. } \relax \relax }{table.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9}Computational complexity}{8}{subsection.2.9}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusion and Discussion}{8}{section.3}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  A: Proof of the First Zonklar Equation}{8}{section*.14}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  B}{8}{section*.15}}
\@writefile{toc}{\contentsline {section}{References}{8}{section*.17}}
\bibcite{ji20133d}{22}
\bibcite{mohamed2012acoustic}{23}
\bibcite{diwucvpr14}{24}
\bibcite{shotton2011real}{25}
\bibcite{lingshao2}{26}
\bibcite{ICMI}{27}
\bibcite{fothergill2012instructing}{28}
\bibcite{guyon2012chalearn}{29}
\bibcite{wang2012mining}{30}
\bibcite{6751269}{31}
\bibcite{nowozin2012action}{32}
\bibcite{bishop2006pattern}{33}
\bibcite{torralba2011unbiased}{34}
\bibcite{mnih2013playing}{35}
\bibcite{wu2012one}{36}
\bibcite{lewis1995fast}{37}
\bibcite{opencv_library}{38}
\@writefile{toc}{\contentsline {section}{Biographies}{9}{IEEEbiography.0}}
\@writefile{toc}{\contentsline {subsection}{Michael Shell}{9}{IEEEbiography.1}}
\@writefile{toc}{\contentsline {subsection}{John Doe}{9}{IEEEbiography.2}}
\@writefile{toc}{\contentsline {subsection}{Jane Doe}{9}{IEEEbiography.3}}
