


The paper has received reviews from three experts who suggest major/minor revisions.  The authors should prepare a major revision that addresses all of the concerns below. I'd be particularly keen to see a comparison with recent landmark-based normalization as suggested by Reviewer 3, and a proper consideration of related work on 3DMMs.

---

Reviewer #1: This paper addresses the problem of 3D gaze estimation. In particular, it solves the problem in a person-independent and head pose-invariant manner.

The key of the proposed method is the use of the depth data. The depth information is used to obtain a 3D face model and then synthesize the frontal appearance. Therefore, the rest appearance-based gaze estimation can be done as if the head pose is stable. In addition, the paper achieves person-independent estimation by using training images from multiple subjects with an eye alignment method.

I would like to know how the authors will address the following comments:
1) It is unclear about the RGB-D sensor's details. How about its accuracy and performance? How will it affect the final accuracy? This should be examined in experimental part.
2) In sec 3.2, it is not clear whether the 3DMM is original. If not, please cite the related papers.
3) This is my major concern: please provide details on the cost to learn the face mesh for each user. In particular, whether it is necessary for every user/every time? What is the time cost? Again, how will it affect the alignment/gaze accuracy?
4) Another major concern: to do eye image alignment, several annotated samples are needed for each user. It is therefore not really calibration-free or person-invariant.
5) The warp function for alignment is described by \theta. It is unclear what is the transformation type. What is the detail of \theta? In L38/L50, the aligned eye images should be the same; can the alignment/transformation with \theta guarentee this? How to handle the difference in color, shape and iris size in different people?
6) There are previous methods based on RGB-D sensors. Comparisons and discussions should be added. 
7) In 'contributions', the authors state that 'almost no previous works address the problem of gaze estimation within a 3D space'. I am not sure whether the authors mention this as a merit. In fact, the difference between 3D direction and 2D position is only a transformation.
8) To examine the person-invariant, leave-one-out experiments are used. This means to use all other subjects' training data for the test subject. I want to see the results by using training samples from no more than half of the subjects.

As mentioned in 3, 4 and 5, I have doubt whether it is proper to consider this method as person-invariant. Since this is the major contribution of this paper, please clearly describe the training cost for each subject. Other comments are also about the novelty of the paper, please address them.


Reviewer #2: This paper describes an approach to gaze estimation from 3D sensors. Using sensors like Kinect, the authors start with an offline step of fitting a 3D Morphable Model to the input point clouds. The 3DMM is a mean shape and a set of deformation vectors M, and each face would then be represented by a set of linear coefficients \alpha. As far as I can tell the paper never mentions the origin of the 3DMM or whether it is possible to learn it from data. The fitting approach is an iteratively re-weighted least squares approach where the correspondences are estimated using ICP given the best fit model, and then the model parameters are estimated using LS given the correspondences. It was not made explicit how the model parameters were initialized or how \lambda_n (the stiffness parameter) is chosen in each iteration. Following the offline setup where a person-specific head model is generated, the system first estimates the head pose for the current frame, then the face
texture is rectified to a front-facing head pose. The eye region is then extracted and aligned to have the eye center in the same location for all the users. The gaze vector is finally estimated using a supervised learning approach, and the authors present multiple options including kNN, linear regression, as well as using a multi-level HOG descriptor + SVR. Estimating the gaze direction as the head pose is also used as a baseline. The training involved presumably using a subject-specific training set, which is limiting, so the authors explore a person-invariant approach where they can use data from all or a subset of the other subjects to train the model. The paper also explores different approaches to eye alignment. The experiments evaluate many of the combination of the algorithms on the EYEDIAP dataset, showing how specific choices improve the gaze estimation error. Additional experiments are performed on the SONVB dataset to find gaze events in job interviews.

Overall the presentation of the paper is clear, and the sections are well-ordered. The technical details are well-presented and sound, but some of the implementation details are missing. Additionally some of the design choices were never evaluated empirically. I believe the following points need to addressed:
1. There was no empirical evaluation of why fitting a person-invariant template (or the mean shape \mu) would not be sufficient for gaze estimation.
2. The value of using the landmark term, which requires additional labeling for each frame
3. How the 3DMM was created, or if it is possible to learn it from data
4. How the model parameters X_0 were initialized and how the \lambda_n were chosen

This is a well-written paper with generally sufficient experiments, but I believe the authors should fill in the missing technical and implementation details, which would also make the work easier to reproduce. I recommend a minor revision.

Typos:
alignement -> alignment (all over)
trainign -> training (line 15, sec 5.1)
Overal -> Overall (line 24, page 14)
Set-up -> Setup (line 52, sec 8)



Reviewer #3: 
The paper presents a system for eye gaze estimation using an RGBD sensor, like the (first generation) Kinect. The approach is to build a person-specific model of the user by fitting a 3DMM to some training frames (using standard method and with the need of manual intervention), then use such person-specific mesh to track the rigid head motion of the user via standard ICP. Given the personalized template the rigid tracking, the authors can extract pose-normalized images of the eyes and employ standard regression techniques to infer the gaze from the eye appearances.

The main contribution over previous work of the same authors is an additional alignment step, where errors in the face model building or in the rigid tracking are alleviated by learning a function which aligns the pose-normalized eye images to the ones used for training.

Experiments are carried out on the EYEDIAP database, which was collected by the same authors and made publicly available. Several methods are compared in several different settings. The results mainly confirm the thesis that trying to normalize for head pose changes increases the accuracy of the gaze estimation.

Even though the problem at hand is important and the work done is of good quality, I find this to be a system paper, with limited novelty, so I consider it to be borderline in terms of importance for a journal such IJCV.

The use of the English language is mostly OK, but there are parts which are not crystal clear, several grammar mistakes, and some inconsistencies. For example, verbs often lack the ending 's' when conjugated in the third person singular, dashes are sometimes used to combine words in a strange, and often inconsistent way (e.g., the text includes both "head pose" and "head-pose", or "person-invariant" and "person invariant"). The paper needs a thorough review in this regard.


More detailed comments, loosely following the paper:

- Page 2, lines 42-44, first column: the authors write "Depth measurements provide information about the shape of a 3D scene...". I disagree: shape information is also present in the RGB data.

- The commercial software faceshift Studio is an obvious competitor to the proposed method: their system, like the one proposed, uses consumer depth cameras (like the Kinect), trains person-specific 3D models fully automatically, and, among other things, outputs the user's eye gaze. I realize that it's not easy to make a quantitative comparison of the two methods (faceshift Studio only processes the live stream coming from the camera and does not allow loading pre-recorded data like those acquired by the authors), but I would at least expect the software to be mentioned in the related work, or a qualitative comparison shown.

- A paper which I think could find its way in the related work is Egger et a., Pose Normalization for Eye Gaze Estimation, 2014, where the authors also use a 3DMM to normalize the eye region of the face before gaze estimation, though only using 2D images.

- The work of Valenti and Gevers, 2012, is listed among the ones which fir an ellipse to the iris, but that is not correct.

- Given today's state-of-the-art methods for 3D face tracking (e.g., faceshift), I would expect a system like the one presented here to run in real time. Instead, I am surprised that here's no mention about the speed of the implementation. Only the ALR alignment method is presented as prohibitively computationally expensive.

- The head pose tracking is really not the main focus of the paper, but, because the same method is used to annotate the database as well as to track the faces in order to remove the head pose, I would suggest that the authors test their 3DMM + ICP method on other publicly available databases of head poses recorded with an RGBD camera, such as the one of Fanelli et al., DAGM'11, (Biwi Kinect Head Pose Database) and of Baltru≈°aitis et al., CVPR'12, (ICT 3D HeadPose Database).

- I find that the words on page 6, lines 23-24, second column, are incorrect: The natural human variations in eye localization should be taken care by the 3DMM fitting; I believe that the need for introducing the proposed, person-specific, alignment transform is only due to the inaccuracy of the 3DMM fitting in the training step, or of the rigid tracking. An interesting experiment would be to artificially and separately control the errors in the 3DMM modeling (i.e., deforming the personalized template so that the eyes are translated away from the right location) and in the rigid 3D tracking and see how the proposed eye alignment methods would cope with such errors.

- The footnote of page 7 says that the cropped eye images are 55x35 pixels in size. But when the EYEDIAP database is presented, the distance of the subjects from the camera makes the eye regions vary between only 13x9 and 19x14 pixels. Why choosing such a larger size for the normalized eye images? Did the authors up-sample the images to get the desired 55x35 images? I wonder what differences in accuracy and speed would be achieved by keeping the images smaller.

- The authors convert the cropped eye images from RGB to greyscale straight away. I wonder whether there is some valuable information in the color channels which is being discarded?

- In section 4.2, regarding ALR, the authors write that the optimization in eq 7 is "conducted for seven predefined values of epsilon". It's not really clear from the text: do the authors mean seven times *per test sample*? This does not seem to be the case for Lu et al 2011a, where a constant value is found using a leave-one-out experiment on the training data.

- In section 4.3, mHoG Features, the authors cite Schneider et al 2014 as supporting the use of mHoG. However, the paper of Schneider reports slightly higher results when mHoG features are used together with LBP. It would be good if the authors also tried LBP as a feature.

- In sec 5.1, a solution is proposed to alleviate the person-generic problem, where a "small" number of frames (how many?) from the test subject are used at test time to select a subset of the subjects in the training database which most closely resemble the person being tracked. For these frames, the whole database needs to be used, so I wonder: how slow is this initialization process? Also, I assume that this "trick" is always used in the ALR experiments, but there's no mention about it after section 5.1.

- In Section 5.2.1, the authors quickly discard using automatic methods for localizing the eye corners and thus align the eye image, claiming that such methods would not work on their low resolution images. I find that there have been quite some steps forward in the localization of facial features (such as eye corners) in recent years (e.g., Cao et al, Face alignment by explicit shape regression, CVPR'12, Dantone et al., Real-time Facial Feature Detection using Conditional Regression Forests, CVPR'12, Kazemi and Sullivan, One Millisecond Face Alignment with an Ensemble of Regression Trees, CVPR'14, etc...), so I wouldn't be so categorical in rejecting an automatic feature-based approach.

- I am a bit concerned about the number of images discarded from the database: given the average clip length of 2.75 minutes and assuming that the Kinect acquired 30fps data, the average clip should contain almost 5000 frames. When the authors indicate as 2400 the average number of frames per session retained after rejecting wrongly annotated frames, that would mean that more than 50% of the recorded frames were discarded. After reading subsection 6.2, I wonder how many of the excluded frames were actually discarded because the automatic ground truth annotation failed, and how reliable in general such annotation procedure was. In page 6 an error of 5mm is mentioned, but it's not clear how that number was computed and also whether it relates to the head pose tracking or to the target tracking. Lastly, samples were also eliminated when "the eye was not fully visible"; what do the authors mean? That when even only one eye was covered, the whole frame was rejected? In general I
think the database would benefit from a more thorough analysis of the quality of the annotation process.

- The way missing values were handled in the DDM method is to simply ignore the pixels or replace them with the average of the rest of the cropped eye region. I wonder if a better solution might be to replace the missing depth values with some approximation (of the depth) based on the neighboring regions: by doing so, the actual RGB data could still be used for the areas where depth sensing was not possible (often the case around the eyes).

- Page 11, first column, at the bottom: the authors write about an additional selection of the test data, done by removing the frames falling outside the convex hull of the training annotations, the motivation is not clearly explained though. A mismatch is mentioned for some subjects in the CS setting, but such mismatch is not well explained nor could I find the percentage of test frames which were excluded in the CS case (this number - 5% - is only mentioned for the FT sessions, where there had been no mismatch).

- I would personally separate the results for the FT and CS conditions, to produce tables which are smaller and easier to understand.

- I appreciate that the authors included the HP experiment, i.e., taking the head direction to be the gaze, but I believe it's enough to show its errors only once, in the tables. In plots like the ones of Fig 8 and 10, reporting the HP error simply makes it very hard to see the differences among the other, more interesting methods.

- It should be highlighted in Fig 8 that the simple kNN achieves lower errors than the other methods for large gaze angles.

- Mean errors are always presented alone, but I would want to see also a measure of the variance of the errors, especially because the differences between the various methods are often very small.

- Fig. 9 shows the error as a function of the number of trainings samples, but I could not find how many samples were used to compute all other errors in the paper. Also, because ALR is so expensive when the number of training data increases, a comparison of the different algorithms in terms of processing speed is needed.

- In the plots of Fig. 10, I am surprised by the distribution of jaw angles, which reach 50 degrees on the right side, but only -20 on the left. I would expect some explanation about this imbalance in the section describing data acquisition.

- Looking at Fig. 6, there are some small artifacts also for two of the images produced by the TDM rectification. Why is that?

- Page 14, lines 17-18, first column: the table number should be mentioned, and Table 1 only has 13 columns, not 14. Here the authors should also explain the EC, A, and A5 acronyms, whose meaning is only mentioned in the caption of Table 1 before this point.

- In Table 1, a difference between A and A5 should be an increased processing speed, so a comparison in such terms is really needed to justify the proposal of the A5 method.

Smaller comments/errors:

Eq 4., the target point u_i is never explained.

Page 9, line 29, second column: why use the word "simplex", when triangle is appropriate (and easier to understand) for the 2D case at hand?

Page 8, line 55, I would not include the case when there is *little* training data available as part of a "person-invariant problem".

Fig 14 should be better explained in the text.

Section 7.2, gaze accuracy, table 2 is mentioned when table 1 is actually meant.
