\documentclass[12pt]{article}
\usepackage{url,graphicx,tabularx,array}
\usepackage{color}
\usepackage[margin=22mm,nohead,nofoot]{geometry}
\begin{document}

\begin{center}
\end{center}

\newcommand{\rev}[1]{{\noindent {\bf Comment:} {\it #1}}~\\}
\newcommand{\ans}[1]{{\noindent {\bf Response:} #1}~\\}


\begin{flushleft}
March 12th, 2015
\end{flushleft}

\vspace*{3mm}

\begin{flushleft}
To: Guest Editor: IJCV special issue on Human Activity Understanding from 2D and 3D cameras
\end{flushleft}

\vspace*{3mm}

\pagestyle{empty}

\noindent Dear Guest Editor,
\newline

This letter is in response to the review of our submitted manuscript 
\#VISI-D-14-00453, entitled {\em Gaze Estimation in the 3D Space Using RGB-D sensors. Towards Head-Pose And User Invariance}.
We would like to thank again the reviewers and guest editor for their time and valuable comments.
We have taken into careful consideration each one of these comments, and have prepared a detailed response
in a separate document adjoint to this letter. Furthermore, this lead to many improvements of the manuscript, as we summarize to you in this letter.

{\em Contributions}: We first would like to recall the contributions of our paper: we have proposed a methodology for 3D gaze estimation
under the very challenging conditions of non collaborative users; directly addressing the problems of head pose
and user(eye) appearance variations. We emphasize the 3D capabilities of the approach
as the addressed range of head pose variations
and gaze directions combined are larger than any previous works on gaze estimation.

We proposed to exploit RGB-D data to track the head pose and to rectify the eye
appearance into a canonical head pose viewpoint, bringing head pose invariance to the appearance based gaze
estimation paradigm, even for important head pose variations. This approach, originally proposed in earlier work of ours, is extensivelly validated in this paper,
using more and different appearance based methods, for diverse tasks and sensing conditions, and using a larger dataset. We have
addressed the problem of user appearance variations through the training of user invariant models for appearance based gaze
estimation. We have proposed a novel eye alignment method which implicitly address the problem of inter-person eyeball center mis-alignment,
circumventing features detection. The user invariant strategy was validated through extensive experiments, in which our proposed
alignment consistenly outperformed strategies based on eye corners{\color{blue}, either manual or automatic}. Finally, we demonstrated
how our method can be used in diverse applications. In particular, we have addressed the problem of automatic gaze coding on natural dyadic
interactions. This application is of {\em high relevance}, as it helps to motivate, further validate  and to demonstrate
our methodology's applicability to challenging scenarios.

{\em Major modifications}: We now would like to summarize the main changes done to the manuscript:

\begin{itemize}
 \item {\em Head pose tracking experiments}: We evaluated the head pose tracker on two publicly available benchmarks: the BIWI Kinect head pose
 dataset and the ICT 3D Head pose dataset. This had two purposes: i) to validate the high accuracy of the head pose tracker, in comparison to representative
 RGB-D methods, as suggested by Rev3 and; ii) to quantitatively and qualitatively confirm the need of the 3DMM offline fitting, by also evaluating
 the usage of a mean face model. The experiments confirm the 3DMM's need for accurate head pose tracking and time-consistent
 eye cropping. This address at once concerns by all reviewers.
 \item {\em Automatic landmarks detection-based experiments}: We have added an experiment in which an state-of-the-art landmarks detection
 algorithm (Kazemi and Sullivan, CVPR2014) is used for the eye alignment within the head frame. The experiments show our proposed
 method also outperforms an alignment based on this automatic eye corners detection. However, notice experiments using {\em manually}
 annotated landmarks subsume such strategies.
 \item {\em Implementation details section}: all reviewers enquired details of our system, such as the 3DMM we used,
 the hyperparameters values, the computational cost of different parts of the methodology and overall speed. We therefore included
 an ``Implementation details'' section where all these elements are described in detail to the reader.
 \item {\em Discussion and future work section}: among the many insighful reviewers comments, there are some which
 are not possible to address directly due to either time constraints, their relevance with respect to the contributions or because
 the data at hand is not adequate. Nevertheless, as a way to address these points, which we considered of interest to
 the reader, we added a ``Discussions and future work'' section. 
\end{itemize}

{\em Minor modifications}: We have addressed all reviewer's comments, most with direct modifications in the paper. We would like to
highlight a few relevant points:
\begin{itemize}
 \item {\em Person-invariance} (cf Rev1): We have clarified the person invariance aspect of our method within the eye image
 to gaze parameters mapping function. Therefore, we better described the contexts  in which the alignment is used,
 which does not contradict the invariance claim.
 \item {\em EYEDIAP}: We clarified to Rev3 and the reader that the automatic annotation process is of high quality. Many frames are ignored because
 the visual target (ground truth) can't be determined. For example, when the ball target leaves the sensor's field of view. EYEDIAP consists
 of {\em non-stop} recordings, where such events are common.
 \item {\em RGB-D based methods} (cf Rev1): We cited and discussed the only and very recent RGB-D based gaze estimation methods.
 These rely on iris ellipse fitting on higher resolution data and much less challenging conditions than in our paper.
\end{itemize}

We would like to address your particular comment: {``\em The paper has received reviews from three experts who suggest major/minor revisions.  The authors should prepare a major revision that addresses all of the concerns below. I'd be particularly keen to see a comparison with recent landmark-based normalization as suggested by Reviewer 3, and a proper consideration of related work on 3DMMs.''}

As mentioned previously, we made a comparison using a recent automatic landmark detection method, as you'll find in the revised
manuscript. We have also considered related works on 3DMM's, as for example, the recent work by Egger et al (2014).{\color{red} todo.. what to say here?}

\vspace*{2mm}
We thank you again for your time and consideration of our manuscript.







\vspace*{8mm}

\noindent Sincerely,\\[3mm]
%
Kenneth Alberto Funes Mora and Jean-Marc Odobez



\newpage


\end{document}



% \ans{Thanks to the Editor and \ldots
% \newline
% \newline
% 
% We have conducted experiments on facial landmarks and the accuracy of the head pose tracker
% 
% Experiment on systematic alignment deviations
% 
% }

