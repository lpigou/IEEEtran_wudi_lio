

\section{Model Formulation}
\label{sec:approach}

\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.48\textwidth]{images/GraphicalModel_new2}\\
  \caption{Per-gesture model: the temporal model is a HMM (left), whose emission probability \emissionprob{} (right) is modeled by a forward-linked chain. The HMM observations \randomvariable{} (skeletal features \randomvariableSK{}, or RGB-D image features \randomvariableRGBD{}) are first passed through deep neural nets: a Deep Belief Network (DBN) for skeletal modality or a 3D convolutional neural network (3DCNN) for the RGB-D modality, to extract high-level features (\highSK{} and \highRGBD{}). The outputs of the neural networks are the emission probabilities of the hidden states $p(X_t | H_t )$.}\label{GM}
\end{figure}

Inspired by the framework successfully applied to speech recognition~\cite{mohamed2012acoustic}, the proposed model is a data driven learning system, relying on a pure learning approach. This results in an integrated model, where the amount of prior knowledge and engineering is minimised. On top of that, this approach works without the need for additional complicated preprocessing and dimensionality reduction methods as it is naturally embedded in the framework.

The proposed approach relies on a Hidden Markov Model (HMM) for the temporal part, where the emission probabilities are modelled by two distinctive types of neural networks appropriate for input modality.
 More specifically, the first model works on skeletal features and the neural network for the emission probabilities is a deep boltzmann machine. The second model, on the other hand, uses convolutional neural networks to model the emission probabilities related to RGB and depth (RGB-D) video data.
 In the remainder of this section, we will first present our temporal model and then introduce its main component. 
The details of the two distinct neural networks and fusion mechanisms along with post-processing will be provided 
in Section~\ref{sec:ModelImplementation}.


\subsection{Deep Dynamic Neural Networks}
\label{sec:DDNN}

The proposed deep dynamic neural network \emph{(DDNN)} can be seen as an extension of~\cite{diwucvpr14}, where instead of only using the restricted Boltzmann machines to model human motion, various connectivity layers (fully connected layers, convolutional layers) are stacked together to learn higher level features justified by a variational bound~\cite{hinton2006fast} from different input modules.

A continuous-observation HMM with discrete hidden states is adopted for modelling higher level temporal relationships. At each time step $t$, we have one observed random variable \randomvariable{} represents the skeleton input \randomvariableSK{} and RGB-D input \randomvariableRGBD{} as shown in the graphical representation in Fig.~\ref{GM}.
 The unobserved variable \hiddenvariable{} taking on values in a finite set composed of \finiteset{}$=(\bigcup _{a \in \mathcal{A}} \mathcal{H}_a)$, where $\mathcal{H}_a$ is a set of states associated with an individual gesture \gesturea{} related to the different gesture. The unobserved variable \hiddenvariable{} can be interpreted as a segment of a gesture \gesturea{}.

 The intuition motivating this construction is that a gesture is composed of a sequence of poses where the relative duration of each pose may vary. This variance is captured by allowing flexible forward transitions within the chain.
 Classically under the HMM assumption, the joint probability of observations and states is given by:
\begin{equation}
p(H_{1:T},X_{1:T}) = p(H_1)p(X_1 | H_1) \prod^{T}_{t=2} p(X_t | H_t ) p(H_t | H_{t-1}),
\label{HMM_GM_1}
\end{equation}
where $p(H_1)$ is the prior on the first hidden state, \transitionmatrix{} is the transition dynamics modelling the allowed state transitions and their probability and $p(X_t | H_t )$ is the emission probability modelled by the deep neural nets.


%\subsection{Ergodic States Hidden Markov Model}
\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.48\textwidth]{images/HMM_2_new}\\
  \caption{
    State diagram of the \emph{ES-HMM} model for low-latency gesture segmentation and recognition. An ergodic state (\emph{$\mathcal{ES}$}) is used to model the resting position between gesture sequences. Each node represents a single state and each row represents a single gesture model. The arrows indicate possible transitions between states.}
    \label{HMM_ES}
\end{figure}
The HMM framework can be used for simultaneous gesture segmentation and recognition.
This is achieved by defining the state transition diagram as shown in Fig~\ref{HMM_ES}. For each given gesture $a \in \mathcal{A}$, a set of state $\mathcal{H}_a$ is introduced to defined a Markov model of that gesture. For example, for action sequence ``tennis serving", the action sequence can be dissected into $h_{a_1}, h_{a_2}, h_{a_3}$ as: 1) raising one arm 2) raising the racket 3) hitting the ball.
More precisely, since our goal is to capture the variation in speed of the performed gestures, we set the transition matrix \transitionmatrix{}  in the following way: when being in a particular node $n$ at time $t$, moving to time $t + 1$, we can either stay in the same node (slower), move to node $n + 1$, or move to node $n+2$ (faster).

Further more, to alow segmentation of gesture, we add an ergodic state (\emph{$\mathcal{ES}$}) which resembles the silence state for speech recognition which serve as a catch-all state.
From the $\mathcal{ES}$ we can move to the first three nodes of any gesture class, and from the last three nodes of any gesture class we can move to the $\mathcal{ES}$.
Hence, the hidden variable \hiddenvariable{} can take values within the finite set, defined as $\mathcal{H}=(\bigcup _{a \in \mathcal{A}} \mathcal{H}_a) \bigcup \{\mathcal{ES}\}$, where $\mathcal{ES}$ is the ergodic state as the resting position between gestures. We refer to the model as the ergodic states hidden Markov model (\emph{ES-HMM}) for simultaneously gesture segmentation and recognition.


The \emph{ES-HMM} framework differs from the firing hidden Markov model of~\cite{nowozin2012action} in that we strictly follow a left-right HMM structure without allowing backward transition, forbidding inter-states transverse, assuming that the considered gestures do not undergo cyclic repetitions as in walking for instance.


%The emission probability is represented as a matrix of size $N_{\mathcal{TC}} \times N_{\mathcal{F}}$ where $N_{\mathcal{F}}$ is the number of frames and output target class $N_{\mathcal{TC}}=N_{\mathcal{A}} \times N_{\mathcal{H}_a}+1$ where $N_{\mathcal{A}}$ is the number of gesture classes and $N_{\mathcal{H}_a}$ is the number of states associated to an individual gesture $a$ and one $\mathcal{ES}$ state (\emph{c.f.} Fig. \ref{Sample0700_comparison}: x-axis as $N_{\mathcal{F}}$ and y-axis as $N_{\mathcal{TC}} $ with $\mathcal{ES}$ as the bottom y-axis 101).


Once we have the trained model, we can use standard techniques to infer online the filtering distribution $p(H_t | X_{1:t})$; or offline the smoothed distribution $p(H_t | X_{1:T})$ where $T$ denotes the end of the sequence.
Because the graph for the hidden Markov model is a directed tree, this problem can be solved exactly and efficiently using the max-sum algorithm also known as Viterbi algorithm. This algorithm searches this space of paths efficiently to find the most probable path with a computational cost that grows only linearly with the length of the chain~\cite{bishop2006pattern}.
%We can infer the gesture presence in a new sequence by Viterbi decoding.
% \begin{equation}
%    V_{t,\mathcal{H}}= \log P(X_t | H_t)+  \log(\max_{\mathcal{H} \in \mathcal{H}_a}( V_{t-1,\mathcal{H}}))
%    \label{viterbi_GDBN}
%\end{equation}
%whith the initial state $V_{1,\mathcal{H}}=P(X_1 |H_1)$.
%From the inference results, we define the probability of a gesture $a \in \mathcal{A}$ as $p(y_t=a|x_{1:t}) =V_{T,\mathcal{H}}$.
The result of the Viterbi algorithm is a path--sequence $h_{t:T}$ of nodes going through the state diagram of Fig.\ref{HMM_ES} and from which we can easily infer the class of the gesture as illustrated in Fig. \ref{Sample0700_comparison}.



\subsection{Learning the emission probability \emissionprob{}}\label{Problem formation}

Traditionally, GMMs and HMMs co-evolved as a way of doing speech recognition when computers were too slow to explore more computationally intensive approaches. GMMs are easy to fit when they have diagonal covariance matrices and, with enough components, they can model any distribution. They are, however, statistically inefficient at modeling high-dimensional data that has many kind of componential structure as explained in~\cite{mohamed2012acoustic}. Suppose, for example, that $\mathcal{N}$ significantly different patterns can occur in one sub-band and $\mathcal{M}$ significantly different patterns can occur in another sub-band. Suppose also that which pattern occurs in each sub-band is approximately independent. A GMM requires $\mathcal{N*M}$ components to model this structure because each component must generate both sub-bands(each piece of data has only a single latent cause). On the other hand, a model that explains the data using multiple causes only requires $\mathcal{N+M}$ components, each of which is specific to a particular sub-band. This exponential inefficiency of GMMs for modeling factorial structures leads to the GMMs+HMMs systems that have a very large number of Gaussians, most of which must be estimated from a very small fraction of the data.

The benefit of learning a generative model is greatly magnified when there is a large supply of unlabeled skeletal, RGB and depth data either acquired by motion capture systems or inferred from depth images in addition to the training data that has been labeled by a forced HMM alignment. We do not make use of unlabeled data in this paper, but it could only improve our results relative to purely discriminatively approaches.

Naturally, many of the high-level features learned by the generative model may be irrelevant for making the required discriminations, even though they are important for explaining the input data. However, this is a price worth paying if computation is cheap and high-level features are very good for discriminating between classes of interest.
The benefit of each weight in a neural network being constrained by a larger faction of training case than each parameter in a GMM has been masked by other differences in training. Neural networks have traditionally been training discriminatively, whereas GMMs are typically trained as generative models (even if discriminative training is performed later in the training procedure). Generative training allows the data to impose many more bits of constraints on the parameters, hence partially compensating for the fact that each component of a large GMM must be trained on a very small fraction of the data.

Feed forward neural networks offer several potential advantages over GMMs:
\begin{itemize}
\item Their estimation of the posterior probabilities of HMM states does not require detailed assumptions about the data distribution.
\item They allow an easy way of combining diverse features, including both discrete and continuous features.
\item They use far more of the data to constrain each parameter because the output on each training case is sensitive to a large fraction of the weights.
\end{itemize}
\textbf{Learning the higher level representation for skeleton joints features}: \label{skeleon_high_level}\newline Neal and Hinton~\cite{neal1998view} demonstrated that the negative log probability of a single data vector, $\textbf{v}^0$, under the multi-layer generative model is bounded by a variational free energy, which is the expected energy under the approximating distribution, $Q(\textbf{h}^0 |\textbf{v}^0 )$, minus the entropy of that distribution. For a directed model, the ``energy" of the configuration $\textbf{v}^0 $,$\textbf{h}^0$ is given by $  E(\textbf{v}^0, \textbf{h}^0) = - [ \log p(\textbf{h}^0)+ \log p(\textbf{v}^0 | \textbf{h}^0)]$.
So the bound is
\begin{align*}
    \log p(\textbf{v}^0) &\geqslant \sum_{\textbf{h}^0} Q(\textbf{h}^0 | \textbf{v}^0) [ \log p (\textbf{h}^0) + \log p (\textbf{v}^0 | \textbf{h}^0)] \\
     &- \sum_{\textbf{h}^0} Q(\textbf{h}^0 | \textbf{v}^0) \log Q(\textbf{h}^0 |\textbf{v}^0)
\end{align*}

The intuition using deep belief networks for modeling marginal distribution \emissionprob{} in skeleton joints action recognition is that by constructing multi-layer networks, semantically meaningful high level features for skeleton configuration will be extracted whilst learning the parametric prior of human pose from mass pool of skeleton joints data. In the recent work of~\cite{6751269} a non-parametric bayesian network is adopted for human pose prior estimation, whereas in our framework, the parametric networks are incorporated.

Using the pair wise joints features as raw input, the data-driven approach network will be able to extract relational multi-joints features which are relevant to target frame class. E.g., for ``toss" action, wrist joints is rotating around shoulder joints would be extracted from the backpropagation via target frame as those task specific, \emph{ad hoc} hard wired sets of joints configurations as in~\cite{chaudhry2013bio}~\cite{muller2006motion}\cite{nowozin2012action}~\cite{ofli2013sequence}.

The outputs of the neural net are the hidden states learned by force alignment during the supervised training process.
Once we have model, we can use the normal online or offline smoothing, inferring the hidden marginal distributions of every node (frame) of the test video.

The overall algorithm for training and testing are presented in Algorithm \ref{MMDDN_train} and \ref{MMDDN_test}.

\begin{algorithm}
\caption{Multimodal deep dynamic networks -- training}\label{MMDDN_train}
\LinesNumbered
\SetAlgoLined
\SetAlgoNoEnd
\DontPrintSemicolon
\SetKwFunction{zeroes}{zeroes}
\KwData{\;
          \inputhmm{}=$[x_{i,1}, \ldots,x_{i,t},\ldots, x_{i,T}]$ \;
          $ \mathbf{X^1=\{ x^1_i\}_{i \in [1 \ldots t]}}$ - raw input (skeletal) feature \; \hspace{1cm} sequence.\;
          $ \mathbf{X^2=\{ x^2_i\}_{i \in [1 \ldots t]}}$ - raw input (depth) feature \; \hspace{1cm} sequence in the form of     $M_1 \times M_2 \times T$, where \; \hspace{1cm} $M_1, M_2$ are the height and width of the input \; \hspace{1cm} image and $T$  is the number of contiguous \; \hspace{1cm} frames of the spatio-temporal cuboid. \;
          $ \mathbf{Y=\{ y_i\}_{i \in [1 \ldots t]}}$  - frame based local label (achieved by\; \hspace{1cm} semi-supervised forced-aligment), where \;
          \hspace{1cm} $ \mathbf{y_i} \in \{ N_{\mathcal{A}} * N_{\mathcal{H}_a} + \textbf{\emph{1}} \} $ with $N_{\mathcal{A}}$ the number of \;
          %\hspace{1cm} classes, $N_{\mathcal{H}_a$ is the number of hidden states for \; \hspace{1cm} each class,
          \hspace{1cm} gesture classes and $N_{\mathcal{H}_a}$ is the number of \;
          \hspace{1cm} states associated to an individual gesture $a$ \;
          \hspace{1cm} and $\textbf{\emph{1}}$ as ergodic state.
            }
%\For{$m \leftarrow 1$ to $2$}{
    \SetAlgoVlined
    %\eIf{$m$ is $1$}{


        Preprocess skeletal data $ \mathbf{X^1}$ as in Eq.\ref{sk_features_1}, \ref{sk_features_2}, \ref{sk_features_3}.\;
        Normalise (zero mean, unit variance per dimension) the above features and feed it to Eq.\ref{GRBMenergy}. \;
        Pre-train the networks using \emph{Contrastive Divergence}. \;
        Supervised fine-tuning of the deep belief networks using $ \mathbf{Y}$ by standard mini-batch \emph{SGD} backpropagation.\;
    %}{
        Preprocess the depth and RGB data $ \mathbf{X^2}$ as in \ref{3d_preproc}.\;
        Feed the above features to Eq.\ref{ReLU}. \;
        Train the 3D convolutional neural networks using $ \mathbf{Y}$.\;
    %}
%}
\KwResult{\;
        $\mathbf{GDBN}$ - a Gaussian-Bernoulli visible layer deep \;
                \hspace{1cm} belief network to generate the emission \;
                \hspace{1cm} probabilities for the hidden Markov model.\;
        $\mathbf{3DCNN}$ - a 3D deep convolutional neural \;
                    \hspace{1cm} network to generate the emission probabilities\;
                    \hspace{1cm} for the hidden Markov model.\;
        $\mathbf{p(X_t | H_t )}$ emission probability. \;
        $\mathbf{p(H_1)}$ - prior probability for $ \mathbf{Y}$ by accumulating and \;
                \hspace{1cm} normalising labels.\;
        $ \mathbf{p(H_t | H_{t-1})}$ - transition probability for $ \mathbf{Y}$,  enforcing\;
                \hspace{1cm} the beginning and ending of a sequence can \;
                \hspace{1cm} only start from the first or the last state.
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t]
\caption{Multimodal deep dynamic networks -- testing}\label{MMDDN_test}
\LinesNumbered
\SetAlgoLined
\SetAlgoNoEnd
\DontPrintSemicolon
\SetKwFunction{zeroes}{zeroes}
\KwData{\;
         $\mathbf{X^1=\{x^1_i\}_{i \in [1 \ldots t]}}$ - raw input (skeletal) feature \; \hspace{1cm} sequence.\;
         $\mathbf{X^2=\{x^2_i\}_{i \in [1 \ldots t]}}$ - raw input (depth) feature \; \hspace{1cm} sequencein the form of $M_1 \times M_2 \times T$. \;
         $\mathbf{GDBN}$ - trained Gaussian-Bernoulli visible layer  \;
                \hspace{1cm} deep belief network to  generate the emission\;
                 \hspace{1cm} probabilities for the hidden Markov model.\;
         $\mathbf{3DCNN}$ - trained  3D deep convolutional neural\;
                    \hspace{1cm} network to generate the emission\;
                    \hspace{1cm} probabilities for the hidden Markov model.\;
        $\mathbf{p(H_1)}$ - prior probability for $ \mathbf{Y}$.\;
        $ \mathbf{p(H_t | H_{t-1})}$ - transition probability for $ \mathbf{Y}.$
            }

%\For{$m \leftarrow 1$ to $2$}{
    \SetAlgoVlined
    %\eIf{$m$ is $1$}{
        Preprocessing and normalising the skeletal data $ \mathbf{X^1}$  as in Eq. \ref{sk_features_1}, \ref{sk_features_2}, \ref{sk_features_3}.\;
        Feedforwarding network $\mathbf{GDBN}$ to generate the emission probability $\mathbf{p(X_t | H_t )}$ in Eq.\ref{HMM_GM_1}. \;
        Generating the score probability matrix $\mathbf{S^1 = p(H_{1:T},X_{1:T}).}$ \;
    %}{
        Preprocessing(median filtering the depth data) and normalising data RGB-D data $ \mathbf{X^2}$ .\;
        Feedforwarding $\mathbf{3DCNN}$ to generate the emission probability $\mathbf{S^2 = p(X_t | H_t )}$ in Eq.\ref{HMM_GM_1}. \;
        Generating the score probability matrix $\mathbf{S^2 =p(H_{1:T},X_{1:T}).}$ \;
    %}
%}
        Fuse the score matrix $\mathbf{S = \alpha * S^1 + (1-\alpha)* S^2}$ OR the learnt joint representation.\;
        Finding the best path $\mathbf{V_{t,\mathcal{H}}}$ using $\mathbf{S}$ by Viterbi decoding. \;
\KwResult{\;
        $ \mathbf{Y=\{ y_i\}_{i \in [1 \ldots t]}}$  - frame based local label  \;
        $ \mathbf{C}$ - global label, the anchor point is chosen as the\;
                        \hspace{1cm} middle state frame.\;
}
\end{algorithm}



\endinput
