\documentclass[10pt,journal,compsoc]{IEEEtran}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{color}
\usepackage[pdftex]{hyperref}
\usepackage{epstopdf}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{mathrsfs}
\usepackage{latexsym}
\usepackage{multirow}
% \usepackage[caption=false]{subfig}
\usepackage[export]{adjustbox}

\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  \usepackage{cite}
\fi


% new command section, Thanks for JMO for the suggestion to keep the document clean and unified.

\include{Newcommand}


\begin{document}

\title{Deep Dynamic Neural Networks for Multimodal Gesture
Segmentation and Recognition}

\author{Di~Wu,
        Lionel~Pigou,
        Pieter-Jan Kindermans,
        Nam~Le,
        Ling~Shao,
        Joni Dambre,
        and~Jean-Marc Odobez
%\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem M. Shell is with the Department
%of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
%GA, 30332.\protect\\
%% note need leading \protect in front of \\ to get a newline within \thanks as
%% \\ is fragile and will error, could use \hfil\break instead.
%E-mail: see http://www.michaelshell.org/contact.html
%\IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.}% <-this % stops an unwanted space
%\thanks{Manuscript received April 19, 2005; revised September 17, 2014.}
}


% The paper headers
\markboth{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, December~2014}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}


\IEEEtitleabstractindextext{%
\begin{abstract}
This paper describes a novel method called deep dynamic neural networks \emph{(DDNN)} for  multimodal gesture recognition.
A semi-supervised hierarchical dynamic framework is proposed for simultaneous gesture segmentation and recognition taking skeleton, depth and RGB images as input observations.
Unlike the traditional construction of complex handcrafted features, all inputs are learnt by deep neural networks: the skeletal module is modelled by deep belief networks \emph{(DBN)}; the depth and RGB module are modelled by 3D convolutional neural networks \emph{(3DCNN)} to extract high-level spatio-temporal features.
The learned representations are then used for estimating emission probabilities of the hidden Markov models to infer a gesture sequence.
The framework can be easily extended by including an ergodic state to segment and recognise video sequences by a frame-to-frame mechanism, making online segmentation and recognition possible.
This purely data driven approach achieves a score of \textbf{\emph{0.81}} in the ChaLearn LAP gesture spotting challenge. The performance is on par with a variety of the state-of-the-art hand-tuned feature approaches and other learning based methods, opening the doors for using deep learning techniques to explore multimodal time series.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Deep learning, convolutional neural networks, deep belief networks, hidden Markov models,
gesture recognition.
\end{IEEEkeywords}}

\maketitle
\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle


%\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\input{Introduction}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{RelatedWork}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Approach}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{ModelDetails}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Results}

\input{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendices
% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{Details of the Code}
The python code using for this work can be found at: \\
\footnotesize{\verb+https://github.com/stevenwudi/chalearn2014_wudi_lio+}


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi

The authors would like to thank Sander Dieleman for his guidance in building, training and initialising convolutional neural networks.


\bibliographystyle{IEEEtran}
\bibliography{tPAMI2015}

\newpage

\newpage

\input{ReviewersComments}


\end{document}
